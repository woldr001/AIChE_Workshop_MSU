{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNQ86lkpCYgwBESn/r1YtHt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"yruyZBNEX4im","executionInfo":{"status":"ok","timestamp":1742756584335,"user_tz":240,"elapsed":140657,"user":{"displayName":"Daniel Woldring","userId":"02599226141182375602"}},"outputId":"c74ec46e-7b82-4209-841e-eb2019f1b8fc"},"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"functional\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │            \u001b[38;5;34m320\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ add (\u001b[38;5;33mAdd\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ multi_head_attention      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │          \u001b[38;5;34m2,160\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention[\u001b[38;5;34m…\u001b[0m │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ add_1 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],             │\n","│                           │                        │                │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ layer_normalization       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │             \u001b[38;5;34m32\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │            \u001b[38;5;34m544\u001b[0m │ layer_normalization[\u001b[38;5;34m0\u001b[0m… │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │            \u001b[38;5;34m528\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ add_2 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ layer_normalization[\u001b[38;5;34m0\u001b[0m… │\n","│                           │                        │                │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ layer_normalization_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │             \u001b[38;5;34m32\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n","│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense_2 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m20\u001b[0m)         │            \u001b[38;5;34m340\u001b[0m │ layer_normalization_1… │\n","└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ multi_head_attention      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,160</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],             │\n","│                           │                        │                │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ layer_normalization       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span> │ layer_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","│                           │                        │                │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ layer_normalization_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n","├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n","│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">340</span> │ layer_normalization_1… │\n","└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,956\u001b[0m (15.45 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,956</span> (15.45 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,956\u001b[0m (15.45 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,956</span> (15.45 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - accuracy: 0.0562 - loss: 3.1822\n","Epoch 2/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.1537 - loss: 2.7338\n","Epoch 3/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.2450 - loss: 2.5757\n","Epoch 4/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.2859 - loss: 2.4463\n","Epoch 5/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.2835 - loss: 2.4370\n","Epoch 6/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.2805 - loss: 2.4297\n","Epoch 7/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.3135 - loss: 2.3554\n","Epoch 8/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.3292 - loss: 2.3276\n","Epoch 9/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.3373 - loss: 2.3036\n","Epoch 10/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.3846 - loss: 2.2483\n","Epoch 11/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4066 - loss: 2.1752\n","Epoch 12/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.4057 - loss: 2.1578\n","Epoch 13/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.4404 - loss: 2.0706\n","Epoch 14/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4124 - loss: 2.1424\n","Epoch 15/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4483 - loss: 2.0334\n","Epoch 16/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4335 - loss: 2.0569\n","Epoch 17/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.4661 - loss: 1.9490\n","Epoch 18/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.4842 - loss: 1.9029\n","Epoch 19/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.4776 - loss: 1.9060\n","Epoch 20/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.4755 - loss: 1.8975\n","Epoch 21/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.4624 - loss: 1.8927\n","Epoch 22/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.4825 - loss: 1.8676\n","Epoch 23/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4517 - loss: 1.9261\n","Epoch 24/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4757 - loss: 1.8454\n","Epoch 25/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5091 - loss: 1.7703\n","Epoch 26/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4884 - loss: 1.8045\n","Epoch 27/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4779 - loss: 1.8570\n","Epoch 28/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5054 - loss: 1.7498\n","Epoch 29/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5084 - loss: 1.7320\n","Epoch 30/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5095 - loss: 1.7342\n","Epoch 31/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4785 - loss: 1.8751\n","Epoch 32/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5173 - loss: 1.7162\n","Epoch 33/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.4924 - loss: 1.7858\n","Epoch 34/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5051 - loss: 1.7269\n","Epoch 35/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5001 - loss: 1.7621\n","Epoch 36/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5297 - loss: 1.6687\n","Epoch 37/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5393 - loss: 1.6417\n","Epoch 38/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5439 - loss: 1.6198\n","Epoch 39/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5068 - loss: 1.7256\n","Epoch 40/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5205 - loss: 1.6871\n","Epoch 41/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5374 - loss: 1.6322\n","Epoch 42/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5443 - loss: 1.6317\n","Epoch 43/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5592 - loss: 1.5709\n","Epoch 44/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5240 - loss: 1.6237\n","Epoch 45/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5266 - loss: 1.6368\n","Epoch 46/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5270 - loss: 1.6393\n","Epoch 47/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5327 - loss: 1.6332\n","Epoch 48/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5383 - loss: 1.5971\n","Epoch 49/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5526 - loss: 1.5876\n","Epoch 50/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5403 - loss: 1.6162\n","Generated Peptide (Transformer): FDAAASKFSAAVLKAAAAAAAEMY\n","Generated Peptide (Transformer): FIAAAAAALAAAAAAIAAAAAAAA\n","Generated Peptide (Transformer): FALAAAAAAAAAAAADAAAAIAYA\n","Generated Peptide (Transformer): FLAAAAATAAKAAAAAAAAAAALA\n","Generated Peptide (Transformer): FLAAAAAAAAAAAAAAAAAAAAAK\n","Generated Peptide (Transformer): FAAAVAAAVAAAAKAAAAAVAAAV\n","Generated Peptide (Transformer): FVAAAAAAAAALAYAAWIAAAEGL\n","Generated Peptide (Transformer): FLAAAAVAAEALAAAAAAAAIAAA\n","Generated Peptide (Transformer): FAQDAVAAAAIAAAAAAAAAAMAA\n","Generated Peptide (Transformer): FAAAAAAAAAAAAAAAAIEAAAAA\n","Generated Peptide (Transformer): FAAAKKAAAAAHVAAAAAAAAIAA\n","Generated Peptide (Transformer): FAAAVAAEAAAAMWPVAAAAAAAA\n","Generated Peptide (Transformer): FLAAAAAAAAAAAAAAVAAAAAAA\n","Generated Peptide (Transformer): FVAVAAAAAAAAAAAAAAAAADIA\n","Generated Peptide (Transformer): FLAAAAAAAAAAAAHVAAAAAAAA\n","Generated Peptide (Transformer): FAAAAAAAAAKALWKAAAAAAAAI\n","Generated Peptide (Transformer): FVSAAAAAAAAAAAAAAAAAAAVE\n","Generated Peptide (Transformer): FLAAIAAAAAAAAAAAIAAAIAAA\n","Generated Peptide (Transformer): FAAAAAAAAAAAAEAAAAAAAAAA\n","Generated Peptide (Transformer): FIAAAAAAAAAAAAAMAAAAAAAA\n","Generated Peptide (Transformer): GAAAAAAAAAKAAAAVAAEAAAAA\n","Generated Peptide (Transformer): GAAAAAAAAAAAAAALAAAAAAAA\n","Generated Peptide (Transformer): GAIAEAAAAIAAAAAAAAAAAKKK\n","Generated Peptide (Transformer): GAYAAAAAAAAAAMADVAAAWKAD\n","Generated Peptide (Transformer): GAAAAAAAAAAAAAAAAAAAAAAA\n","Generated Peptide (Transformer): GAAAILAAVAKAAAAAAAAAAAAA\n","Generated Peptide (Transformer): GAAAAAAAAAYAAEAAAAAAAAAI\n","Generated Peptide (Transformer): GAAAAAAAAAAAAAAAAAAAAAAA\n","Generated Peptide (Transformer): GAAYMEYAAAAAHKKAIAAAAAAK\n","Generated Peptide (Transformer): GAAAAAAAAAVAAAAAAAAAAAAA\n","Generated Peptide (Transformer): GAFAAAAAAAAAAAAAAAAAAAAA\n","Generated Peptide (Transformer): GAAAAAAAAAAAAAAAAAAAAEAA\n","Generated Peptide (Transformer): GAAAAAAAAAAAAAHAAAAAIAAA\n","Generated Peptide (Transformer): GAAAAAAAAAAAADLAAAAAAEAA\n","Generated Peptide (Transformer): GAAAAAAAAAAAAEAAAEWIALAA\n","Generated Peptide (Transformer): GAAAAAAALAEYAAAAVAAAAAAA\n","Generated Peptide (Transformer): GAAAAAAAAAAAAAAAAAAALAAA\n","Generated Peptide (Transformer): GAAAAAAAAAAAAAAAAAAAIAAA\n","Generated Peptide (Transformer): GAAAAAAAAAAAAAAAAAKAHEKK\n","Generated Peptide (Transformer): GAEAEAAAAAAAAYNIAAAAAAEK\n"]}],"source":["\"\"\"\n","Transformer Generative Model for Antimicrobial Peptides\n","=======================================================\n","\n","This script demonstrates a simplified Transformer for generating short protein sequences.\n","\"\"\"\n","\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Model\n","\n","# 1. Example AMP dataset\n","'''\n","amp_sequences: list of the AMP protein sequences the model will learn from.\n","'''\n","amp_sequences = [\n","            'FLPLLAGLAANFLPTIICKISYKC',\n","            'FLPFIARLAAKVFPSIICSVTKKC',\n","            'GVLSNVIGYLKKLGTGALNAVLKQ',\n","            'GLFSVLGAVAKHVLPHVVPVIAEK',\n","            'GLFKVLGSVAKHLLPHVAPVIAEK',\n","            'GLFKVLGSVAKHLLPHVVPVIAEK',\n","            'GLFGVLGSIAKHVLPHVVPVIAEK',\n","            'MFFSSKKCKTVSKTFRGPCVRNAN',\n","            'LLKELWTKMKGAGKAVLGKIKGLL',\n","            'LLKELWTKIKGAGKAVLGKIKGLL',\n","            'FWGALIKGAAKLIPSVVGLFKKKQ',\n","            'FLPVVAGLAAKVLPSIICAVTKKC',\n","            'FLPAIVGAAGQFLPKIFCAISKKC',\n","            'FLPAIVGAAGKFLPKIFCAISKKC',\n","            'FFPIVAGVAGQVLKKIYCTISKKC',\n","            'FLPIIAGIAAKVFPKIFCAISKKC',\n","            'FLPMLAGLAASMVPKLVCLITKKC',\n","            'FLPMLAGLAASMVPKFVCLITKKC',\n","            'FLPFIAGMAAKFLPKIFCAISKKC',\n","            'FLPAIAGMAAKFLPKIFCAISKKC',\n","            'FLPFIAGVAAKFLPKIFCAISKKC',\n","            'FLPAIAGVAAKFLPKIFCAISKKC',\n","            'FLPAIVGAAAKFLPKIFCVISKKC',\n","            'FLPFIAGMAANFLPKIFCAISKKC',\n","            'FLPIIAGVAAKVFPKIFCAISKKC',\n","            'FLPIIASVAAKVFSKIFCAISKKC',\n","            'FLPIIASVAANVFSKIFCAISKKC',\n","            'GLNTLKKVFQGLHEAIKLINNHVQ',\n","            'GLNALKKVFQGIHEAIKLINNHVQ',\n","            'DSHAKRHHGYKRKFHEKHHSHRGY',\n","            'FLPLLAGLAANFLPKIFCKITKKC',\n","            'FLPILAGLAAKIVPKLFCLATKKC',\n","            'FLPLIAGLAANFLPKIFCAITKKC',\n","            'FLPVIAGVAAKFLPKIFCAITKKC',\n","            'FWGALAKGALKLIPSLFSSFSKKD',\n","            'ITSVSWCTPGCTSEGGGSGCSHCC',\n","            'GLLNGLALRLGKRALKKIIKRLCR',\n","            'ALWKDILKNAGKAALNEINQLVNQ',\n","            'GLRSKIWLWVLLMIWQESNKFKKM',\n","            'GKGRWLERIGKAGGIIIGGALDHL',\n","            'FLGALIKGAIHGGRFIHGMIQNHH',\n","            'FLGLLFHGVHHVGKWIHGLIHGHH',\n","            'FLPMLAGLAANFLPKLFCKITKKC',\n","            'FLPLAVSLAANFLPKLFCKITKKC',\n","            'FLPLLAGLAANFFPKIFCKITRKC',\n","            'FLPILASLAAKFGPKLFCLVTKKC',\n","            'FLPILASLAAKLGPKLFCLVTKKC',\n","            'FLPILASLAATLGPKLLCLITKKC',\n","            'GIFSNMYARTPAGYFRGPAGYAAN',\n","            'GLKDKFKSMGEKLKQYIQTWKAKF',\n","            'SLKDKVKSMGEKLKQYIQTWKAKF',\n","            'GFRDVLKGAAKAFVKTVAGHIANI',\n","            'GIKDWIKGAAKKLIKTVASNIANQ',\n","            'GFKDWIKGAAKKLIKTVASSIANQ',\n","            'VIPFVASVAAEMMQHVYCAASKKC',\n","            'FFGTALKIAANVLPTAICKILKKC',\n","            'FFGTALKIAANILPTAICKILKKC',\n","            'ILPFVAGVAAEMMQHVYCAASKKC',\n","            'FLPAIVGAAAKFLPKIFCAISKKC',\n","            'FLPIIAGVAAKVLPKIFCAISKKC',\n","            'FLPIIAGIAAKFLPKIFCTISKKC',\n","            'FLPVIAGVAANFLPKLFCAISKKC',\n","            'FLPIIAGAAAKVVQKIFCAISKKC',\n","            'FLPIIAGAAAKVVEKIFCAISKKC',\n","            'FLPAVLRVAAKIVPTVFCAISKKC',\n","            'FLPAVLRVAAQVVPTVFCAISKKC',\n","            'FMGGLIKAATKIVPAAYCAITKKC',\n","            'FLPILAGLAAKLVPKVFCSITKKC',\n","            'FLPILAGLAANILPKVFCSITKKC',\n","            'FFPIIAGMAAKLIPSLFCKITKKC',\n","            'FMGSALRIAAKVLPAALCQIFKKC',\n","            'DSHEKRHHEHRRKFHEKHHSHRGY',\n","            'WRSLGRTLLRLSHALKPLARRSGW',\n","            'VTSWSLCTPGCTSPGGGSNCSFCC',\n","            'VIPFVASVAAEMMHHVYCAASKRC',\n","            'SPAGCRFCCGCCPNMRGCGVCCRF',\n","            'GRGREFMSNLKEKLSGVKEKMKNS',\n","            'FLPVLTGLTPSIVPKLVCLLTKKC',\n","            'FLPVLAGLTPSIVPKLVCLLTKKC',\n","            'FFPMLAGVAARVVPKVICLITKKC',\n","            'DSMGAVKLAKLLIDKMKCEVTKAC',\n","            'FLPGVLRLVTKVGPAVVCAITRNC',\n","            'VIVFVASVAAEMMQHVYCAASKKC',\n","            'FLPAVIRVAANVLPTAFCAISKKC',\n","            'IDPFVAGVAAEMMQHVYCAASKKC',\n","            'INPFVAGVAAEMMQHVYCAASKKC',\n","            'ILPFVAGVAAEMMKHVYCAASKKC',\n","            'IIPFVAGVAAEMMEHVYCAASKKC',\n","            'QLPFVAGVACEMCQCVYCAASKKC',\n","            'ILPFVAGVAAEMMEHVYCAASKKC',\n","            'ILPFVAGVAAMEMEHVYCAASKKC',\n","            'FLPAVLLVATHVLPTVFCAITRKC',\n","            'IPWKLPATFRPVERPFSKPFCRKD',\n","            'FLPLLAGVVANFLPQIICKIARKC',\n","            'FLGSLLGLVGKVVPTLFCKISKKC',\n","            'FIGPVLKIAAGILPTAICKIFKKC',\n","            'FVGPVLKIAAGILPTAICKIYKKC',\n","            'FLGPIIKIATGILPTAICKFLKKC',\n","            'FLPLIASLAANFVPKIFCKITKKC',\n","            'FLPLIASVAANLVPKIFCKITKKC',\n","            'FLSTLLKVAFKVVPTLFCPITKKC',\n","            'KRKCPKTPFDNTPGAWFAHLILGC',\n","            'FLGLIFHGLVHAGKLIHGLIHRNR',\n","            'FLPAVIRVAANVLPTVFCAISKKC',\n","            'FLPAVLRVAAKVVPTVFCLISKKC',\n","            'FLSTALKVAANVVPTLFCKITKKC',\n","            'FLPIVAGLAANFLPKIVCKITKKC',\n","            'FLSTLLNVASNVVPTLICKITKKC',\n","            'FLSTLLNVASKVVPTLFCKITKKC',\n","            'FLPMLAGLAANFLPKIVCKITKKC',\n","            'FIGPVLKMATSILPTAICKGFKKC',\n","            'FLGPIIKMATGILPTAICKGLKKC',\n","            'FLPIIAGVAAKVLPKLFCAITKKC',\n","            'FLPVIAGLAAKVLPKLFCAITKKC',\n","            'RKGWFKAMKSIAKFIAKEKLKEHL',\n","            'FLPAVLKVAAHILPTAICAISRRC',\n","            'FMGTALKIAANVLPAAFCKIFKKC',\n","            'KLGFENFLVKALKTVMHVPTSPLL',\n","            'GWLPTFGKILRKAMQLGPKLIQPI',\n","            'GNGVVLTLTHECNLATWTKKLKCC',\n","            'ITIPPIVKNTLKKFIKGAVSALMS',\n","            'FLPGLIKAAVGVGSTILCKITKKC',\n","            'FLPGLIKAAVGIGSTIFCKISKKC',\n","            'FLPGLIKVAVGVGSTILCKITKKC',\n","            'FLPGLIKAAVGIGSTIFCKISRKC',\n","            'FLPMLAGLAANFLPKIICKITKKC',\n","            'FLPIVASLAANFLPKIICKITKKC',\n","            'FWGALAKGALKLIPSLVSSFTKKD',\n","            'FFPLIAGLAARFLPKIFCSITKRC',\n","            'VIPFVASVAAEMMQHVYCAASKRC',\n","            'FFPSIAGLAAKFLPKIFCSITKRC',\n","            'FLPAVLRVAAKVGPAVFCAITQKC',\n","            'FLGMLLHGVGHAIHGLIHGKQNVE',\n","            'NPAGCRFCCGCCPNMIGCGVCCRF',\n","            'IWSFLIKAATKLLPSLFGGGKKDS',\n","            'RNGCIVDPRCPYQQCRRPLYCRRR',\n","            'ILELAGNAARDNKKTRIIPRHLQL',\n","            'FLPLLAGLAANFLPTIICKIARKC',\n","            'FLPAIIGMAAKVLPAFLCKITKKC',\n","            'RRRRRFRRVIRRIRLPKYLTINTE',\n","            'GNGVLKTISHECNMNTWQFLFTCC',\n","            'FLPILAGLAANLVPKLICSITKKC',\n","            'FLGAVLKVAGKLVPAAICKISKKC',\n","            'FLGALFKVASKLVPAAICSISKKC',\n","            'FLPVIAGIAANVLPKLFCKLTKRC',\n","            'FFPIIARLAAKVIPSLVCAVTKKC',\n","            'KRVNWRKVGRNTALGASYVLSFLG',\n","            'GHSVDRIPEYFGPPGLPGPVLFYS',\n","            'FLPLIAGVAAKVLPKIFCAISKKC',\n","            'SDSVVSDIICTTFCSVTWCQSNCC',\n","            'FLPLLAGLAANFLPQIICKIARKC',\n","            'FLGTVLKVAAKVLPAALCQIFKKC',\n","            'QSHLSMCRYCCCKGNKGCGFCCKF',\n","            'VFDIIKDAGKQLVAHAMGKIAEKV',\n","            'VFDIIKDAGRQLVAHAMGKIAEKV',\n","            'FLPLLAGLAASFLPTIFCKISRKC',\n","            'FFPIVAGVAAKVLKKIFCTISKKC',\n","    # ...\n","]\n","\n","\n","# 2. Build character-to-index mapping\n","'''\n","unique_amino_acids: This extracts all the unique characters (amino acid letters) from the sequences.\n","char_to_idx: This dictionary maps each amino acid character to a unique numerical index (e.g., 'F' might be 0, 'L' might be 1, etc.).\n","idx_to_char: This dictionary does the reverse, mapping numerical indices back to amino acid characters.\n","vocab_size: This stores the total number of unique amino acids in the dataset.\n","'''\n","unique_amino_acids = sorted(list(set(\"\".join(amp_sequences))))\n","char_to_idx = {char: idx for idx, char in enumerate(unique_amino_acids)}\n","idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n","vocab_size = len(unique_amino_acids)\n","\n","\n","# 3. Convert to integer arrays\n","'''\n","encoded_sequences: This converts the original protein sequences (amp_sequences) into numerical representations using the char_to_idx mapping.\n","  Each amino acid is replaced with its corresponding index.\n","seq_length: This sets the maximum length of the sequences the model will handle\n","  (24 amino acids in this case).\n","X and y: These are created to train the model. X contains the input sequences\n","  (all but the last amino acid), and y contains the target sequences (all but the first amino acid). This setup is for next-token prediction, where the model learns to predict the next amino acid in a sequence.\n","'''\n","encoded_sequences = []\n","for seq in amp_sequences:\n","    encoded_sequences.append([char_to_idx[c] for c in seq])\n","encoded_sequences = np.array(encoded_sequences)  # shape: (num_sequences, seq_length)\n","\n","# Prepare training data for next-token prediction\n","seq_length = 24\n","X = encoded_sequences[:, :-1]  # shape: (num_sequences, seq_length-1)\n","y = encoded_sequences[:, 1:]   # shape: (num_sequences, seq_length-1)\n","\n","\n","# 4. Build a small Transformer model\n","''' We'll define the input, embedding, transformer block, and final dense layer.\n","This section defines the architecture of the Transformer model using TensorFlow's Keras API.\n","embedding_dim, num_heads, ff_dim: These are hyperparameters that control the size and complexity of the model.\n","The model consists of an input layer, an embedding layer (to represent amino acids as vectors),\n","  a positional encoding layer (to provide information about the order of amino acids),\n","  a transformer encoder block (the core of the model for learning relationships between amino acids),\n","  and a final dense layer (to output predictions for the next amino acid).\n","model.compile: This configures the model for training, specifying the optimizer (adam),\n","  loss function (sparse_categorical_crossentropy), and metrics to track (accuracy).\n","model.summary(): This displays a summary of the model's architecture.\n","'''\n","\n","embedding_dim = 16\n","num_heads = 2\n","ff_dim = 32  # feed-forward layer size in transformer\n","\n","# Define Input\n","inputs = layers.Input(shape=(seq_length-1,))  # each example is length-1 = 23\n","\n","# Token Embedding + Positional Embedding\n","token_embedding = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)(inputs)\n","\n","# Basic positional encoding\n","positions = tf.range(start=0, limit=seq_length-1, delta=1)\n","positional_encoding = layers.Embedding(input_dim=seq_length, output_dim=embedding_dim)(positions)\n","positional_encoding = positional_encoding[None, ...]  # shape: (1, seq_length-1, embedding_dim)\n","\n","# Add token embedding and positional encoding\n","x = token_embedding + positional_encoding\n","\n","# Transformer Encoder Block (simplified)\n","'''\n","This is where self-attention is handled\n","The model uses query, key, and attention weighting, although implicitly.\n","The layers.MultiHeadAttention layer handles these steps internally.\n","By passing x as both the query and the key/value (using (x, x)), the model is essentially performing self-attention, comparing different parts of the input sequence with itself.\n","The key_dim argument specifies the dimensionality of the keys and queries, influencing the complexity of the attention calculations.\n","'''\n","attention_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(x, x)\n","attention_output = layers.Dropout(0.1)(attention_output)\n","x = layers.LayerNormalization(epsilon=1e-6)(x + attention_output)\n","\n","ffn = layers.Dense(ff_dim, activation='relu')(x)\n","ffn = layers.Dense(embedding_dim)(ffn)\n","ffn = layers.Dropout(0.1)(ffn)\n","x = layers.LayerNormalization(epsilon=1e-6)(x + ffn)\n","\n","# Final Dense Layer over vocab\n","outputs = layers.Dense(vocab_size, activation='softmax')(x)\n","\n","model = Model(inputs=inputs, outputs=outputs)\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","model.summary()\n","\n","\n","# 5. Train the Transformer\n","'''\n","epochs: The number of times the model will go through the entire training data.\n","batch_size: The number of training examples processed in each iteration.\n","model.fit: This starts the training process, using the prepared data (X, y) and the specified training parameters.\n","'''\n","epochs = 50\n","batch_size = 16\n","model.fit(X, y, epochs=epochs, batch_size=batch_size)\n","\n","\n","# 6. Generation function\n","'''\n","generate_transformer_sequence: This function takes the trained model and a starting amino acid (start_token) and generates a new peptide sequence of the specified length.\n","It works by repeatedly predicting the next amino acid based on the previous ones, using the model's learned knowledge.\n","The example usage demonstrates how to generate 20 new sequences starting with 'F' and 20 starting with 'G'.\n","'''\n","\n","\n","def generate_transformer_sequence(model, start_token, length=24):\n","    \"\"\"\n","    Generate a new peptide sequence from a transformer model.\n","    :param model: trained Keras model\n","    :param start_token: integer index of first amino acid\n","    :param length: desired total length\n","    :return: generated amino acid sequence (string)\n","    \"\"\"\n","    generated = [start_token]\n","\n","    for i in range(length-1):\n","        # We feed the current sequence (minus 1 for next-token prediction)\n","        input_seq = np.array(generated)[None, ...]  # shape: (1, current_length)\n","\n","        # Model expects length=23 for training; in generation we can adapt.\n","        # We'll zero-pad to length=23 for simplicity (or you can dynamically mask).\n","        pad_len = (seq_length - 1) - len(generated)\n","        if pad_len < 0:\n","            # If your sequence is already at length=23, we only use the last 23 tokens\n","            input_seq = np.array(generated[-(seq_length-1):])[None, ...]\n","            pad_len = 0\n","\n","        input_seq = np.pad(input_seq, ((0,0),(0,pad_len)), 'constant', constant_values=0)\n","\n","        preds = model.predict(input_seq, verbose=0)\n","        # We want the last position's distribution\n","        last_pos = len(generated)-1 if len(generated) < (seq_length-1) else (seq_length-2)\n","        prob_dist = preds[0, last_pos]  # shape: (vocab_size,)\n","\n","        next_idx = np.random.choice(range(vocab_size), p=prob_dist)\n","        generated.append(next_idx)\n","\n","    # Convert to string\n","    generated_str = \"\".join(idx_to_char[idx] for idx in generated)\n","    return generated_str\n","\n","# Example usage:\n","for i in range(20):\n","    start_token = char_to_idx['F']\n","    new_peptide = generate_transformer_sequence(model, start_token, length=24)\n","    print(\"Generated Peptide (Transformer):\", new_peptide)\n","\n","for i in range(20):\n","    start_token = char_to_idx['G']\n","    new_peptide = generate_transformer_sequence(model, start_token, length=24)\n","    print(\"Generated Peptide (Transformer):\", new_peptide)"]}]}