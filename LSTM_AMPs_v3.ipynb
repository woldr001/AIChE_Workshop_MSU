{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNbpVmROtmXgawoRYfeACUu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woldr001/AIChE_Workshop_MSU/blob/main/LSTM_AMPs_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generative ML Models in Protein Engineering"
      ],
      "metadata": {
        "id": "uOHGqsJLQF4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM Generative Model for Antimicrobial Peptides\n",
        "================================================\n",
        "\n",
        "This script shows how to:\n",
        "\n",
        "1.   Preprocess AMP sequences (tokenize amino acids).\n",
        "2.   Train an LSTM-based model to predict the next amino acid.\n",
        "3.   Generate new sequences by sampling from the trained model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xzt4xP36qzmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "TNwqxr6Qr1nR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Antimicrobial Peptide (AMP) Sequence Dataset\n",
        "Note: With only ~150 AMP sequences (each length 24), overfitting is likely.\n",
        "      Consider data augmentation, dropout, or pretraining on larger protein sets."
      ],
      "metadata": {
        "id": "uJZUyR1WsBoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "amp_sequences = [\n",
        "            'FLPLLAGLAANFLPTIICKISYKC',\n",
        "            'FLPFIARLAAKVFPSIICSVTKKC',\n",
        "            'GVLSNVIGYLKKLGTGALNAVLKQ',\n",
        "            'GLFSVLGAVAKHVLPHVVPVIAEK',\n",
        "            'GLFKVLGSVAKHLLPHVAPVIAEK',\n",
        "            'GLFKVLGSVAKHLLPHVVPVIAEK',\n",
        "            'GLFGVLGSIAKHVLPHVVPVIAEK',\n",
        "            'MFFSSKKCKTVSKTFRGPCVRNAN',\n",
        "            'LLKELWTKMKGAGKAVLGKIKGLL',\n",
        "            'LLKELWTKIKGAGKAVLGKIKGLL',\n",
        "            'FWGALIKGAAKLIPSVVGLFKKKQ',\n",
        "            'FLPVVAGLAAKVLPSIICAVTKKC',\n",
        "            'FLPAIVGAAGQFLPKIFCAISKKC',\n",
        "            'FLPAIVGAAGKFLPKIFCAISKKC',\n",
        "            'FFPIVAGVAGQVLKKIYCTISKKC',\n",
        "            'FLPIIAGIAAKVFPKIFCAISKKC',\n",
        "            'FLPMLAGLAASMVPKLVCLITKKC',\n",
        "            'FLPMLAGLAASMVPKFVCLITKKC',\n",
        "            'FLPFIAGMAAKFLPKIFCAISKKC',\n",
        "            'FLPAIAGMAAKFLPKIFCAISKKC',\n",
        "            'FLPFIAGVAAKFLPKIFCAISKKC',\n",
        "            'FLPAIAGVAAKFLPKIFCAISKKC',\n",
        "            'FLPAIVGAAAKFLPKIFCVISKKC',\n",
        "            'FLPFIAGMAANFLPKIFCAISKKC',\n",
        "            'FLPIIAGVAAKVFPKIFCAISKKC',\n",
        "            'FLPIIASVAAKVFSKIFCAISKKC',\n",
        "            'FLPIIASVAANVFSKIFCAISKKC',\n",
        "            'GLNTLKKVFQGLHEAIKLINNHVQ',\n",
        "            'GLNALKKVFQGIHEAIKLINNHVQ',\n",
        "            'DSHAKRHHGYKRKFHEKHHSHRGY',\n",
        "            'FLPLLAGLAANFLPKIFCKITKKC',\n",
        "            'FLPILAGLAAKIVPKLFCLATKKC',\n",
        "            'FLPLIAGLAANFLPKIFCAITKKC',\n",
        "            'FLPVIAGVAAKFLPKIFCAITKKC',\n",
        "            'FWGALAKGALKLIPSLFSSFSKKD',\n",
        "            'ITSVSWCTPGCTSEGGGSGCSHCC',\n",
        "            'GLLNGLALRLGKRALKKIIKRLCR',\n",
        "            'ALWKDILKNAGKAALNEINQLVNQ',\n",
        "            'GLRSKIWLWVLLMIWQESNKFKKM',\n",
        "            'GKGRWLERIGKAGGIIIGGALDHL',\n",
        "            'FLGALIKGAIHGGRFIHGMIQNHH',\n",
        "            'FLGLLFHGVHHVGKWIHGLIHGHH',\n",
        "            'FLPMLAGLAANFLPKLFCKITKKC',\n",
        "            'FLPLAVSLAANFLPKLFCKITKKC',\n",
        "            'FLPLLAGLAANFFPKIFCKITRKC',\n",
        "            'FLPILASLAAKFGPKLFCLVTKKC',\n",
        "            'FLPILASLAAKLGPKLFCLVTKKC',\n",
        "            'FLPILASLAATLGPKLLCLITKKC',\n",
        "            'GIFSNMYARTPAGYFRGPAGYAAN',\n",
        "            'GLKDKFKSMGEKLKQYIQTWKAKF',\n",
        "            'SLKDKVKSMGEKLKQYIQTWKAKF',\n",
        "            'GFRDVLKGAAKAFVKTVAGHIANI',\n",
        "            'GIKDWIKGAAKKLIKTVASNIANQ',\n",
        "            'GFKDWIKGAAKKLIKTVASSIANQ',\n",
        "            'VIPFVASVAAEMMQHVYCAASKKC',\n",
        "            'FFGTALKIAANVLPTAICKILKKC',\n",
        "            'FFGTALKIAANILPTAICKILKKC',\n",
        "            'ILPFVAGVAAEMMQHVYCAASKKC',\n",
        "            'FLPAIVGAAAKFLPKIFCAISKKC',\n",
        "            'FLPIIAGVAAKVLPKIFCAISKKC',\n",
        "            'FLPIIAGIAAKFLPKIFCTISKKC',\n",
        "            'FLPVIAGVAANFLPKLFCAISKKC',\n",
        "            'FLPIIAGAAAKVVQKIFCAISKKC',\n",
        "            'FLPIIAGAAAKVVEKIFCAISKKC',\n",
        "            'FLPAVLRVAAKIVPTVFCAISKKC',\n",
        "            'FLPAVLRVAAQVVPTVFCAISKKC',\n",
        "            'FMGGLIKAATKIVPAAYCAITKKC',\n",
        "            'FLPILAGLAAKLVPKVFCSITKKC',\n",
        "            'FLPILAGLAANILPKVFCSITKKC',\n",
        "            'FFPIIAGMAAKLIPSLFCKITKKC',\n",
        "            'FMGSALRIAAKVLPAALCQIFKKC',\n",
        "            'DSHEKRHHEHRRKFHEKHHSHRGY',\n",
        "            'WRSLGRTLLRLSHALKPLARRSGW',\n",
        "            'VTSWSLCTPGCTSPGGGSNCSFCC',\n",
        "            'VIPFVASVAAEMMHHVYCAASKRC',\n",
        "            'SPAGCRFCCGCCPNMRGCGVCCRF',\n",
        "            'GRGREFMSNLKEKLSGVKEKMKNS',\n",
        "            'FLPVLTGLTPSIVPKLVCLLTKKC',\n",
        "            'FLPVLAGLTPSIVPKLVCLLTKKC',\n",
        "            'FFPMLAGVAARVVPKVICLITKKC',\n",
        "            'DSMGAVKLAKLLIDKMKCEVTKAC',\n",
        "            'FLPGVLRLVTKVGPAVVCAITRNC',\n",
        "            'VIVFVASVAAEMMQHVYCAASKKC',\n",
        "            'FLPAVIRVAANVLPTAFCAISKKC',\n",
        "            'IDPFVAGVAAEMMQHVYCAASKKC',\n",
        "            'INPFVAGVAAEMMQHVYCAASKKC',\n",
        "            'ILPFVAGVAAEMMKHVYCAASKKC',\n",
        "            'IIPFVAGVAAEMMEHVYCAASKKC',\n",
        "            'QLPFVAGVACEMCQCVYCAASKKC',\n",
        "            'ILPFVAGVAAEMMEHVYCAASKKC',\n",
        "            'ILPFVAGVAAMEMEHVYCAASKKC',\n",
        "            'FLPAVLLVATHVLPTVFCAITRKC',\n",
        "            'IPWKLPATFRPVERPFSKPFCRKD',\n",
        "            'FLPLLAGVVANFLPQIICKIARKC',\n",
        "            'FLGSLLGLVGKVVPTLFCKISKKC',\n",
        "            'FIGPVLKIAAGILPTAICKIFKKC',\n",
        "            'FVGPVLKIAAGILPTAICKIYKKC',\n",
        "            'FLGPIIKIATGILPTAICKFLKKC',\n",
        "            'FLPLIASLAANFVPKIFCKITKKC',\n",
        "            'FLPLIASVAANLVPKIFCKITKKC',\n",
        "            'FLSTLLKVAFKVVPTLFCPITKKC',\n",
        "            'KRKCPKTPFDNTPGAWFAHLILGC',\n",
        "            'FLGLIFHGLVHAGKLIHGLIHRNR',\n",
        "            'FLPAVIRVAANVLPTVFCAISKKC',\n",
        "            'FLPAVLRVAAKVVPTVFCLISKKC',\n",
        "            'FLSTALKVAANVVPTLFCKITKKC',\n",
        "            'FLPIVAGLAANFLPKIVCKITKKC',\n",
        "            'FLSTLLNVASNVVPTLICKITKKC',\n",
        "            'FLSTLLNVASKVVPTLFCKITKKC',\n",
        "            'FLPMLAGLAANFLPKIVCKITKKC',\n",
        "            'FIGPVLKMATSILPTAICKGFKKC',\n",
        "            'FLGPIIKMATGILPTAICKGLKKC',\n",
        "            'FLPIIAGVAAKVLPKLFCAITKKC',\n",
        "            'FLPVIAGLAAKVLPKLFCAITKKC',\n",
        "            'RKGWFKAMKSIAKFIAKEKLKEHL',\n",
        "            'FLPAVLKVAAHILPTAICAISRRC',\n",
        "            'FMGTALKIAANVLPAAFCKIFKKC',\n",
        "            'KLGFENFLVKALKTVMHVPTSPLL',\n",
        "            'GWLPTFGKILRKAMQLGPKLIQPI',\n",
        "            'GNGVVLTLTHECNLATWTKKLKCC',\n",
        "            'ITIPPIVKNTLKKFIKGAVSALMS',\n",
        "            'FLPGLIKAAVGVGSTILCKITKKC',\n",
        "            'FLPGLIKAAVGIGSTIFCKISKKC',\n",
        "            'FLPGLIKVAVGVGSTILCKITKKC',\n",
        "            'FLPGLIKAAVGIGSTIFCKISRKC',\n",
        "            'FLPMLAGLAANFLPKIICKITKKC',\n",
        "            'FLPIVASLAANFLPKIICKITKKC',\n",
        "            'FWGALAKGALKLIPSLVSSFTKKD',\n",
        "            'FFPLIAGLAARFLPKIFCSITKRC',\n",
        "            'VIPFVASVAAEMMQHVYCAASKRC',\n",
        "            'FFPSIAGLAAKFLPKIFCSITKRC',\n",
        "            'FLPAVLRVAAKVGPAVFCAITQKC',\n",
        "            'FLGMLLHGVGHAIHGLIHGKQNVE',\n",
        "            'NPAGCRFCCGCCPNMIGCGVCCRF',\n",
        "            'IWSFLIKAATKLLPSLFGGGKKDS',\n",
        "            'RNGCIVDPRCPYQQCRRPLYCRRR',\n",
        "            'ILELAGNAARDNKKTRIIPRHLQL',\n",
        "            'FLPLLAGLAANFLPTIICKIARKC',\n",
        "            'FLPAIIGMAAKVLPAFLCKITKKC',\n",
        "            'RRRRRFRRVIRRIRLPKYLTINTE',\n",
        "            'GNGVLKTISHECNMNTWQFLFTCC',\n",
        "            'FLPILAGLAANLVPKLICSITKKC',\n",
        "            'FLGAVLKVAGKLVPAAICKISKKC',\n",
        "            'FLGALFKVASKLVPAAICSISKKC',\n",
        "            'FLPVIAGIAANVLPKLFCKLTKRC',\n",
        "            'FFPIIARLAAKVIPSLVCAVTKKC',\n",
        "            'KRVNWRKVGRNTALGASYVLSFLG',\n",
        "            'GHSVDRIPEYFGPPGLPGPVLFYS',\n",
        "            'FLPLIAGVAAKVLPKIFCAISKKC',\n",
        "            'SDSVVSDIICTTFCSVTWCQSNCC',\n",
        "            'FLPLLAGLAANFLPQIICKIARKC',\n",
        "            'FLGTVLKVAAKVLPAALCQIFKKC',\n",
        "            'QSHLSMCRYCCCKGNKGCGFCCKF',\n",
        "            'VFDIIKDAGKQLVAHAMGKIAEKV',\n",
        "            'VFDIIKDAGRQLVAHAMGKIAEKV',\n",
        "            'FLPLLAGLAASFLPTIFCKISRKC',\n",
        "            'FFPIVAGVAAKVLKKIFCTISKKC',\n",
        "    # AMP sequences, each of length 24\n",
        "]"
      ],
      "metadata": {
        "id": "MYns1hrSsbD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Build a character-to-index mapping\n",
        "Depending on your data, you might have 20 canonical amino acids + special tokens if needed."
      ],
      "metadata": {
        "id": "K54IOXj0smTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_amino_acids = sorted(list(set(\"\".join(amp_sequences))))\n",
        "# e.g., unique_amino_acids might look like: [\"A\", \"C\", \"D\", \"E\", ..., \"Y\"]\n",
        "\n",
        "char_to_idx = {char: idx for idx, char in enumerate(unique_amino_acids)}\n",
        "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
        "\n",
        "vocab_size = len(unique_amino_acids)  # e.g., could be 20 if strictly canonical\n"
      ],
      "metadata": {
        "id": "8Gk_J2S-tD03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Convert sequences to integer arrays"
      ],
      "metadata": {
        "id": "pXRerRa-tb0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_sequences = []\n",
        "for seq in amp_sequences:\n",
        "    encoded_sequences.append([char_to_idx[c] for c in seq])\n",
        "\n",
        "encoded_sequences = np.array(encoded_sequences)  # shape: (num_sequences, seq_length)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1ZaHpW-PtcOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Prepare training data\n",
        "   We can train a \"next-character prediction\" model.   \n",
        "   We can treat the amino acid sequence as a tiime series.    \n",
        "   For each position t in a sequence, predict the amino acid at position t+1.       \n",
        "   We'll \"shift\" the sequence by 1 for targets.    \n",
        "   Input: [X_0, X_1, ..., X_{22}],   \n",
        "   Target: [X_1, X_2, ..., X_{23}].\n",
        "   \n",
        "   We do this for all sequences."
      ],
      "metadata": {
        "id": "vdOPgwV9tn1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = encoded_sequences[:, :-1]  # all but last character\n",
        "y = encoded_sequences[:, 1:]   # all but first character"
      ],
      "metadata": {
        "id": "iSVoze6-tsDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Define LSTM model\n",
        "\n",
        "**Sequential**: This creates a linear stack of layers to build the LSTM model.  \n",
        "\n",
        "**Embedding**: This layer converts each amino acid index into a dense vector representation (embedding) of size embedding_dim. This allows the model to capture relationships between amino acids.  \n",
        "\n",
        "**LSTM**: This is the core layer, learning long-term dependencies in the sequence data. lstm_units sets the dimensionality of the LSTM's hidden state.\n",
        "return_sequences=True makes the LSTM output a sequence for each input sequence,\n",
        "  necessary for predicting the next amino acid at each position.  \n",
        "\n",
        "**Dense**: This is the output layer, with vocab_size neurons. It uses the 'softmax'\n",
        "  activation to produce a probability distribution over all possible amino acids,\n",
        "  representing the model's prediction for the next amino acid in the sequence.  \n",
        "\n",
        "**Adam**: An optimization algorithm that helps the model learn more effectively.  \n",
        "\n",
        "**compile**: Configures the model for training, specifying the loss function, optimizer, and evaluation metrics.  \n",
        "\n",
        "**model.summary()**: Prints a summary of the model's architecture.\n"
      ],
      "metadata": {
        "id": "83hil2MtuhRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "# Embedding layer: (vocab_size) distinct amino acid characters -> embedding_dim vectors\n",
        "embedding_dim = 8\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=23))\n",
        "\n",
        "# LSTM layer\n",
        "lstm_units = 64\n",
        "model.add(LSTM(lstm_units, return_sequences=True))\n",
        "\n",
        "# Final Dense layer for classification over the vocabulary\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "FQZ0iwdPuhij",
        "outputId": "bbe65a6c-11a2-473e-de49-a5bbbce32e0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Train the model\n",
        "*Note: Because the dataset is small, this is primarily an illustrative example.*\n",
        "\n",
        "**X** and **y**: Represent the input and target data for training.\n",
        "  X contains the encoded AMP sequences shifted by one position,\n",
        "  and y contains the original encoded sequences shifted by one position to the right,\n",
        "  so the model learns to predict the next amino acid in the sequence.  \n",
        "**epochs**: The number of times the model sees the entire training dataset.  \n",
        "**batch_size**: The number of samples processed before the model's internal parameters are updated.  \n",
        "**model.fit**: Starts the training process.\n"
      ],
      "metadata": {
        "id": "9RDM9rczvWQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "batch_size = 16\n",
        "model.fit(X, y, epochs=epochs, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "11gg8CWnvWbs",
        "outputId": "b013d360-4248-4774-c866-9a52e1baca77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.1079 - loss: 2.8716\n",
            "Epoch 2/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.1861 - loss: 2.6210\n",
            "Epoch 3/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.2077 - loss: 2.5446\n",
            "Epoch 4/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.2239 - loss: 2.4219\n",
            "Epoch 5/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.2669 - loss: 2.3424\n",
            "Epoch 6/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.3628 - loss: 2.1469\n",
            "Epoch 7/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.4198 - loss: 1.9869\n",
            "Epoch 8/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.4685 - loss: 1.8082\n",
            "Epoch 9/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5069 - loss: 1.6442\n",
            "Epoch 10/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5227 - loss: 1.5588\n",
            "Epoch 11/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5269 - loss: 1.5127\n",
            "Epoch 12/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5555 - loss: 1.4529\n",
            "Epoch 13/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5459 - loss: 1.4696\n",
            "Epoch 14/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6004 - loss: 1.3135\n",
            "Epoch 15/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5997 - loss: 1.2828\n",
            "Epoch 16/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5787 - loss: 1.3528\n",
            "Epoch 17/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6170 - loss: 1.2312\n",
            "Epoch 18/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6482 - loss: 1.1235\n",
            "Epoch 19/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6602 - loss: 1.1149\n",
            "Epoch 20/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6559 - loss: 1.1241\n",
            "Epoch 21/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6703 - loss: 1.0575\n",
            "Epoch 22/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6832 - loss: 1.0316\n",
            "Epoch 23/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6944 - loss: 1.0098\n",
            "Epoch 24/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6975 - loss: 0.9841\n",
            "Epoch 25/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7271 - loss: 0.8907\n",
            "Epoch 26/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7493 - loss: 0.8147\n",
            "Epoch 27/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7782 - loss: 0.7528\n",
            "Epoch 28/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7642 - loss: 0.7878\n",
            "Epoch 29/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7794 - loss: 0.7816\n",
            "Epoch 30/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7960 - loss: 0.7183\n",
            "Epoch 31/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7793 - loss: 0.7402\n",
            "Epoch 32/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7873 - loss: 0.7057\n",
            "Epoch 33/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7959 - loss: 0.6750\n",
            "Epoch 34/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8166 - loss: 0.6256\n",
            "Epoch 35/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8138 - loss: 0.6243\n",
            "Epoch 36/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8375 - loss: 0.5491\n",
            "Epoch 37/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8480 - loss: 0.5426\n",
            "Epoch 38/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8555 - loss: 0.5205\n",
            "Epoch 39/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8562 - loss: 0.4996\n",
            "Epoch 40/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8570 - loss: 0.4895\n",
            "Epoch 41/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8671 - loss: 0.4763\n",
            "Epoch 42/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8573 - loss: 0.4845\n",
            "Epoch 43/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8607 - loss: 0.4728\n",
            "Epoch 44/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8679 - loss: 0.4677\n",
            "Epoch 45/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8635 - loss: 0.4497\n",
            "Epoch 46/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8851 - loss: 0.4077\n",
            "Epoch 47/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8839 - loss: 0.3983\n",
            "Epoch 48/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8967 - loss: 0.3853\n",
            "Epoch 49/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8934 - loss: 0.3697\n",
            "Epoch 50/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8986 - loss: 0.3589\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7eaa10b12dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Generating new sequences\n",
        "\n",
        "**generate_sequence**: This function takes the trained model, a starting sequence (seed_seq),\n",
        "  and a desired sequence length as input. It uses the model to predict the next amino acid step-by-step, generating a new sequence.  \n",
        "**seed**: The starting point for sequence generation, in this case, the amino acid 'F'.  \n",
        "The loop runs 20 times, generating and printing 20 new AMP sequences.\n"
      ],
      "metadata": {
        "id": "ogLMBfZOvsVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sequence(model, seed_seq, length=24):\n",
        "    \"\"\"\n",
        "    Generate a new sequence of desired length using the trained model.\n",
        "    :param model: trained LSTM model\n",
        "    :param seed_seq: list of integer-encoded amino acids (starting sequence)\n",
        "    :param length: desired total length of generated sequence\n",
        "    :return: string of amino acids\n",
        "    \"\"\"\n",
        "    generated = seed_seq[:]  # copy\n",
        "\n",
        "    for _ in range(length - len(seed_seq)):\n",
        "        # Predict next amino acid distribution\n",
        "        input_seq = np.array(generated[-1:])  # last amino acid as input\n",
        "        input_seq = input_seq.reshape(1, -1)  # shape: (1, 1)\n",
        "\n",
        "        # Model expects a fixed input length of 23 for each training example,\n",
        "        # so for generation, we can adapt in different ways.\n",
        "        # Simplest approach: pad/truncate to length=23 and only use last token for the next prediction\n",
        "        # We'll do a simple approach:\n",
        "        padded_seq = np.zeros((1, 23))\n",
        "        padded_seq[0, 22] = input_seq[0, 0]\n",
        "\n",
        "        # Predict\n",
        "        preds = model.predict(padded_seq, verbose=0)[0, 22, :]\n",
        "\n",
        "        next_idx = np.random.choice(range(vocab_size), p=preds)\n",
        "        generated.append(next_idx)\n",
        "\n",
        "    # Convert generated integer tokens to string\n",
        "    generated_str = \"\".join(idx_to_char[idx] for idx in generated)\n",
        "    return generated_str\n",
        "\n",
        "# Example usage:\n",
        "# Start generation from a single amino acid: 'F'\n",
        "seed = [char_to_idx['F']]  # or choose any valid token from your vocab\n",
        "for i in range(20):\n",
        "    new_peptide = generate_sequence(model, seed, length=24)\n",
        "    print(\"Generated Peptide:\", new_peptide)"
      ],
      "metadata": {
        "id": "NWkcJIZIvscB",
        "outputId": "77fb3fb2-e23e-46cc-f46f-4defe5f31ced",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Peptide: FTLDLIIIDIELIDDIIIDIIDID\n",
            "Generated Peptide: FDIIDIIIIIDIDIIDIIDIDDII\n",
            "Generated Peptide: FDIDIIDIIDDDVIIIDIDVDIDI\n",
            "Generated Peptide: FDIDIDIIDMIDIIDDIDIDIDLI\n",
            "Generated Peptide: FDIDLIIIDYDIIIDIDIIIDIDI\n",
            "Generated Peptide: FDWDLIIIIDIIDIIDIDIDIDID\n",
            "Generated Peptide: FDIDIDIDIDIIDDIDIIDIIDDI\n",
            "Generated Peptide: FDIIIDIDLDIRLDIIIIDIIFDI\n",
            "Generated Peptide: FNIIDIDDIIIIIDIDIDIDIIDI\n",
            "Generated Peptide: FDIDIIELIIDIIIIDIIIIDIDL\n",
            "Generated Peptide: FDDIDIIIDIDIIDIIDIDIDDID\n",
            "Generated Peptide: FDDIDIDIIDIIIDLNDILDIDLD\n",
            "Generated Peptide: FDIVDDIDIDDIIDIDDIIDIDID\n",
            "Generated Peptide: FDIDIWDIDIIELDDIDIIDIDLD\n",
            "Generated Peptide: FDIIDIDIDIIIIIDIDDIDDIID\n",
            "Generated Peptide: FDIDLIDIDIIDIDDIDIIDIDII\n",
            "Generated Peptide: FDIDIDIDIDIDIIDIDIIIIDII\n",
            "Generated Peptide: FDIIIDIIDMLDIIDIIIDIDIII\n",
            "Generated Peptide: FDLDIIIIDLIIRLFDIDIDFDSL\n",
            "Generated Peptide: FDIDIIDLDIVIDDIIIDIIDIYD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LSTM Generated Sequences:\n",
        "\n"
      ],
      "metadata": {
        "id": "_z7lj0AsrWqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "FLLRYYLRFRYLRFLRYLRYYYYL\n",
        "FLCRYYCRYYYLRRYCNYFLNLNL\n",
        "FLRFCRYYYLGYYLLRFRYYLRRY\n",
        "FLRYCRYYFGLLRYLRYLRYCRYL\n",
        "FLRYLRYYYYCRFLRFLRYCRYLR\n",
        "FYCRFRYLRKYLRYYYLRYYYRFR\n",
        "FCNRYRYRYYRYYYLLFRYYYYLR\n",
        "FLCRYCRYYYYYLRFSRFRRYYCR\n",
        "FCRYCRYRYLCLRCRRYYLRRYLR\n",
        "FLRLRLRRYLRLLRYCRFLRYYYL\n",
        "FRYRYLRRYYFYCRLCRYLRYCRY\n",
        "FCRYYYYRYCRYLRYYYLGYLRYL\n",
        "FRYYLRFLRFCRRLRYLCRCRYRY\n",
        "FLRYYYCRRYCRYCKYLGYCRRFR\n",
        "FCRYCRYMNYFLRFLRYRFRYYYF\n",
        "FRYYRYYYYYLLRYYYRRRCRYCR\n",
        "FCNLYCRFCRFLRCLRYYYCRYRY\n",
        "FCRRYLRYYYYYYYCRYLRYLRYY\n",
        "FCRYMNLRRRLRYYLCRYCRYRYR\n",
        "FLRRYYRCNRFLRYFYYLRYLRRY\n",
        "'''"
      ],
      "metadata": {
        "id": "qcgiLLCZk4oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#New\n"
      ],
      "metadata": {
        "id": "TNx2SwNVBi3S"
      }
    }
  ]
}