{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOxI9yY/yEqrwkyOKqJo0jF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"fNLllR5f2apV","executionInfo":{"status":"ok","timestamp":1742579983295,"user_tz":240,"elapsed":75750,"user":{"displayName":"Daniel Woldring","userId":"02599226141182375602"}},"outputId":"6cc0446e-cb68-41eb-9246-22a782388c74"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.1121 - loss: 2.8813\n","Epoch 2/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.1955 - loss: 2.5743\n","Epoch 3/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.2207 - loss: 2.4862\n","Epoch 4/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.2384 - loss: 2.4447\n","Epoch 5/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.3113 - loss: 2.2447\n","Epoch 6/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.3391 - loss: 2.1580\n","Epoch 7/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.4090 - loss: 2.0077\n","Epoch 8/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.4239 - loss: 1.9185\n","Epoch 9/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.4894 - loss: 1.6927\n","Epoch 10/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.4859 - loss: 1.6775\n","Epoch 11/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5234 - loss: 1.5080\n","Epoch 12/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5413 - loss: 1.4467\n","Epoch 13/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5642 - loss: 1.3870\n","Epoch 14/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5586 - loss: 1.4320\n","Epoch 15/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5850 - loss: 1.3216\n","Epoch 16/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6021 - loss: 1.2637\n","Epoch 17/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6069 - loss: 1.2366\n","Epoch 18/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6044 - loss: 1.2545\n","Epoch 19/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6338 - loss: 1.1685\n","Epoch 20/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6338 - loss: 1.1603\n","Epoch 21/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6583 - loss: 1.0930\n","Epoch 22/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6635 - loss: 1.0589\n","Epoch 23/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6725 - loss: 1.0514\n","Epoch 24/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7029 - loss: 0.9605\n","Epoch 25/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6995 - loss: 0.9403\n","Epoch 26/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7225 - loss: 0.8816\n","Epoch 27/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7195 - loss: 0.8784\n","Epoch 28/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7175 - loss: 0.9111\n","Epoch 29/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7278 - loss: 0.8880\n","Epoch 30/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7501 - loss: 0.7914\n","Epoch 31/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7502 - loss: 0.8082\n","Epoch 32/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7657 - loss: 0.7646\n","Epoch 33/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7705 - loss: 0.7357\n","Epoch 34/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7650 - loss: 0.7272\n","Epoch 35/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7764 - loss: 0.7468\n","Epoch 36/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7846 - loss: 0.7165\n","Epoch 37/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7970 - loss: 0.6666\n","Epoch 38/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8156 - loss: 0.6095\n","Epoch 39/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8277 - loss: 0.5844\n","Epoch 40/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8419 - loss: 0.5613\n","Epoch 41/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8421 - loss: 0.5397\n","Epoch 42/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8450 - loss: 0.5386\n","Epoch 43/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8499 - loss: 0.4913\n","Epoch 44/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8679 - loss: 0.4746\n","Epoch 45/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8746 - loss: 0.4654\n","Epoch 46/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8706 - loss: 0.4523\n","Epoch 47/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8806 - loss: 0.4273\n","Epoch 48/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8852 - loss: 0.4197\n","Epoch 49/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8747 - loss: 0.4240\n","Epoch 50/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8902 - loss: 0.3926\n","Generated Peptide: FLLRYYLRFRYLRFLRYLRYYYYL\n","Generated Peptide: FLCRYYCRYYYLRRYCNYFLNLNL\n","Generated Peptide: FLRFCRYYYLGYYLLRFRYYLRRY\n","Generated Peptide: FLRYCRYYFGLLRYLRYLRYCRYL\n","Generated Peptide: FLRYLRYYYYCRFLRFLRYCRYLR\n","Generated Peptide: FYCRFRYLRKYLRYYYLRYYYRFR\n","Generated Peptide: FCNRYRYRYYRYYYLLFRYYYYLR\n","Generated Peptide: FLCRYCRYYYYYLRFSRFRRYYCR\n","Generated Peptide: FCRYCRYRYLCLRCRRYYLRRYLR\n","Generated Peptide: FLRLRLRRYLRLLRYCRFLRYYYL\n","Generated Peptide: FRYRYLRRYYFYCRLCRYLRYCRY\n","Generated Peptide: FCRYYYYRYCRYLRYYYLGYLRYL\n","Generated Peptide: FRYYLRFLRFCRRLRYLCRCRYRY\n","Generated Peptide: FLRYYYCRRYCRYCKYLGYCRRFR\n","Generated Peptide: FCRYCRYMNYFLRFLRYRFRYYYF\n","Generated Peptide: FRYYRYYYYYLLRYYYRRRCRYCR\n","Generated Peptide: FCNLYCRFCRFLRCLRYYYCRYRY\n","Generated Peptide: FCRRYLRYYYYYYYCRYLRYLRYY\n","Generated Peptide: FCRYMNLRRRLRYYLCRYCRYRYR\n","Generated Peptide: FLRRYYRCNRFLRYFYYLRYLRRY\n"]}],"source":["\"\"\"\n","LSTM Generative Model for Antimicrobial Peptides\n","================================================\n","\n","This script shows how to:\n","1) Preprocess AMP sequences (tokenize amino acids).\n","2) Train an LSTM-based model to predict the next amino acid.\n","3) Generate new sequences by sampling from the trained model.\n","\n","Note: With only ~150 AMP sequences (each length 24), overfitting is likely.\n","      Consider data augmentation, dropout, or pretraining on larger protein sets.\n","\"\"\"\n","\n","import numpy as np\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","from tensorflow.keras.optimizers import Adam\n","\n","# 1. Example dataset of AMP sequences\n","amp_sequences = [\n","            'FLPLLAGLAANFLPTIICKISYKC',\n","            'FLPFIARLAAKVFPSIICSVTKKC',\n","            'GVLSNVIGYLKKLGTGALNAVLKQ',\n","            'GLFSVLGAVAKHVLPHVVPVIAEK',\n","            'GLFKVLGSVAKHLLPHVAPVIAEK',\n","            'GLFKVLGSVAKHLLPHVVPVIAEK',\n","            'GLFGVLGSIAKHVLPHVVPVIAEK',\n","            'MFFSSKKCKTVSKTFRGPCVRNAN',\n","            'LLKELWTKMKGAGKAVLGKIKGLL',\n","            'LLKELWTKIKGAGKAVLGKIKGLL',\n","            'FWGALIKGAAKLIPSVVGLFKKKQ',\n","            'FLPVVAGLAAKVLPSIICAVTKKC',\n","            'FLPAIVGAAGQFLPKIFCAISKKC',\n","            'FLPAIVGAAGKFLPKIFCAISKKC',\n","            'FFPIVAGVAGQVLKKIYCTISKKC',\n","            'FLPIIAGIAAKVFPKIFCAISKKC',\n","            'FLPMLAGLAASMVPKLVCLITKKC',\n","            'FLPMLAGLAASMVPKFVCLITKKC',\n","            'FLPFIAGMAAKFLPKIFCAISKKC',\n","            'FLPAIAGMAAKFLPKIFCAISKKC',\n","            'FLPFIAGVAAKFLPKIFCAISKKC',\n","            'FLPAIAGVAAKFLPKIFCAISKKC',\n","            'FLPAIVGAAAKFLPKIFCVISKKC',\n","            'FLPFIAGMAANFLPKIFCAISKKC',\n","            'FLPIIAGVAAKVFPKIFCAISKKC',\n","            'FLPIIASVAAKVFSKIFCAISKKC',\n","            'FLPIIASVAANVFSKIFCAISKKC',\n","            'GLNTLKKVFQGLHEAIKLINNHVQ',\n","            'GLNALKKVFQGIHEAIKLINNHVQ',\n","            'DSHAKRHHGYKRKFHEKHHSHRGY',\n","            'FLPLLAGLAANFLPKIFCKITKKC',\n","            'FLPILAGLAAKIVPKLFCLATKKC',\n","            'FLPLIAGLAANFLPKIFCAITKKC',\n","            'FLPVIAGVAAKFLPKIFCAITKKC',\n","            'FWGALAKGALKLIPSLFSSFSKKD',\n","            'ITSVSWCTPGCTSEGGGSGCSHCC',\n","            'GLLNGLALRLGKRALKKIIKRLCR',\n","            'ALWKDILKNAGKAALNEINQLVNQ',\n","            'GLRSKIWLWVLLMIWQESNKFKKM',\n","            'GKGRWLERIGKAGGIIIGGALDHL',\n","            'FLGALIKGAIHGGRFIHGMIQNHH',\n","            'FLGLLFHGVHHVGKWIHGLIHGHH',\n","            'FLPMLAGLAANFLPKLFCKITKKC',\n","            'FLPLAVSLAANFLPKLFCKITKKC',\n","            'FLPLLAGLAANFFPKIFCKITRKC',\n","            'FLPILASLAAKFGPKLFCLVTKKC',\n","            'FLPILASLAAKLGPKLFCLVTKKC',\n","            'FLPILASLAATLGPKLLCLITKKC',\n","            'GIFSNMYARTPAGYFRGPAGYAAN',\n","            'GLKDKFKSMGEKLKQYIQTWKAKF',\n","            'SLKDKVKSMGEKLKQYIQTWKAKF',\n","            'GFRDVLKGAAKAFVKTVAGHIANI',\n","            'GIKDWIKGAAKKLIKTVASNIANQ',\n","            'GFKDWIKGAAKKLIKTVASSIANQ',\n","            'VIPFVASVAAEMMQHVYCAASKKC',\n","            'FFGTALKIAANVLPTAICKILKKC',\n","            'FFGTALKIAANILPTAICKILKKC',\n","            'ILPFVAGVAAEMMQHVYCAASKKC',\n","            'FLPAIVGAAAKFLPKIFCAISKKC',\n","            'FLPIIAGVAAKVLPKIFCAISKKC',\n","            'FLPIIAGIAAKFLPKIFCTISKKC',\n","            'FLPVIAGVAANFLPKLFCAISKKC',\n","            'FLPIIAGAAAKVVQKIFCAISKKC',\n","            'FLPIIAGAAAKVVEKIFCAISKKC',\n","            'FLPAVLRVAAKIVPTVFCAISKKC',\n","            'FLPAVLRVAAQVVPTVFCAISKKC',\n","            'FMGGLIKAATKIVPAAYCAITKKC',\n","            'FLPILAGLAAKLVPKVFCSITKKC',\n","            'FLPILAGLAANILPKVFCSITKKC',\n","            'FFPIIAGMAAKLIPSLFCKITKKC',\n","            'FMGSALRIAAKVLPAALCQIFKKC',\n","            'DSHEKRHHEHRRKFHEKHHSHRGY',\n","            'WRSLGRTLLRLSHALKPLARRSGW',\n","            'VTSWSLCTPGCTSPGGGSNCSFCC',\n","            'VIPFVASVAAEMMHHVYCAASKRC',\n","            'SPAGCRFCCGCCPNMRGCGVCCRF',\n","            'GRGREFMSNLKEKLSGVKEKMKNS',\n","            'FLPVLTGLTPSIVPKLVCLLTKKC',\n","            'FLPVLAGLTPSIVPKLVCLLTKKC',\n","            'FFPMLAGVAARVVPKVICLITKKC',\n","            'DSMGAVKLAKLLIDKMKCEVTKAC',\n","            'FLPGVLRLVTKVGPAVVCAITRNC',\n","            'VIVFVASVAAEMMQHVYCAASKKC',\n","            'FLPAVIRVAANVLPTAFCAISKKC',\n","            'IDPFVAGVAAEMMQHVYCAASKKC',\n","            'INPFVAGVAAEMMQHVYCAASKKC',\n","            'ILPFVAGVAAEMMKHVYCAASKKC',\n","            'IIPFVAGVAAEMMEHVYCAASKKC',\n","            'QLPFVAGVACEMCQCVYCAASKKC',\n","            'ILPFVAGVAAEMMEHVYCAASKKC',\n","            'ILPFVAGVAAMEMEHVYCAASKKC',\n","            'FLPAVLLVATHVLPTVFCAITRKC',\n","            'IPWKLPATFRPVERPFSKPFCRKD',\n","            'FLPLLAGVVANFLPQIICKIARKC',\n","            'FLGSLLGLVGKVVPTLFCKISKKC',\n","            'FIGPVLKIAAGILPTAICKIFKKC',\n","            'FVGPVLKIAAGILPTAICKIYKKC',\n","            'FLGPIIKIATGILPTAICKFLKKC',\n","            'FLPLIASLAANFVPKIFCKITKKC',\n","            'FLPLIASVAANLVPKIFCKITKKC',\n","            'FLSTLLKVAFKVVPTLFCPITKKC',\n","            'KRKCPKTPFDNTPGAWFAHLILGC',\n","            'FLGLIFHGLVHAGKLIHGLIHRNR',\n","            'FLPAVIRVAANVLPTVFCAISKKC',\n","            'FLPAVLRVAAKVVPTVFCLISKKC',\n","            'FLSTALKVAANVVPTLFCKITKKC',\n","            'FLPIVAGLAANFLPKIVCKITKKC',\n","            'FLSTLLNVASNVVPTLICKITKKC',\n","            'FLSTLLNVASKVVPTLFCKITKKC',\n","            'FLPMLAGLAANFLPKIVCKITKKC',\n","            'FIGPVLKMATSILPTAICKGFKKC',\n","            'FLGPIIKMATGILPTAICKGLKKC',\n","            'FLPIIAGVAAKVLPKLFCAITKKC',\n","            'FLPVIAGLAAKVLPKLFCAITKKC',\n","            'RKGWFKAMKSIAKFIAKEKLKEHL',\n","            'FLPAVLKVAAHILPTAICAISRRC',\n","            'FMGTALKIAANVLPAAFCKIFKKC',\n","            'KLGFENFLVKALKTVMHVPTSPLL',\n","            'GWLPTFGKILRKAMQLGPKLIQPI',\n","            'GNGVVLTLTHECNLATWTKKLKCC',\n","            'ITIPPIVKNTLKKFIKGAVSALMS',\n","            'FLPGLIKAAVGVGSTILCKITKKC',\n","            'FLPGLIKAAVGIGSTIFCKISKKC',\n","            'FLPGLIKVAVGVGSTILCKITKKC',\n","            'FLPGLIKAAVGIGSTIFCKISRKC',\n","            'FLPMLAGLAANFLPKIICKITKKC',\n","            'FLPIVASLAANFLPKIICKITKKC',\n","            'FWGALAKGALKLIPSLVSSFTKKD',\n","            'FFPLIAGLAARFLPKIFCSITKRC',\n","            'VIPFVASVAAEMMQHVYCAASKRC',\n","            'FFPSIAGLAAKFLPKIFCSITKRC',\n","            'FLPAVLRVAAKVGPAVFCAITQKC',\n","            'FLGMLLHGVGHAIHGLIHGKQNVE',\n","            'NPAGCRFCCGCCPNMIGCGVCCRF',\n","            'IWSFLIKAATKLLPSLFGGGKKDS',\n","            'RNGCIVDPRCPYQQCRRPLYCRRR',\n","            'ILELAGNAARDNKKTRIIPRHLQL',\n","            'FLPLLAGLAANFLPTIICKIARKC',\n","            'FLPAIIGMAAKVLPAFLCKITKKC',\n","            'RRRRRFRRVIRRIRLPKYLTINTE',\n","            'GNGVLKTISHECNMNTWQFLFTCC',\n","            'FLPILAGLAANLVPKLICSITKKC',\n","            'FLGAVLKVAGKLVPAAICKISKKC',\n","            'FLGALFKVASKLVPAAICSISKKC',\n","            'FLPVIAGIAANVLPKLFCKLTKRC',\n","            'FFPIIARLAAKVIPSLVCAVTKKC',\n","            'KRVNWRKVGRNTALGASYVLSFLG',\n","            'GHSVDRIPEYFGPPGLPGPVLFYS',\n","            'FLPLIAGVAAKVLPKIFCAISKKC',\n","            'SDSVVSDIICTTFCSVTWCQSNCC',\n","            'FLPLLAGLAANFLPQIICKIARKC',\n","            'FLGTVLKVAAKVLPAALCQIFKKC',\n","            'QSHLSMCRYCCCKGNKGCGFCCKF',\n","            'VFDIIKDAGKQLVAHAMGKIAEKV',\n","            'VFDIIKDAGRQLVAHAMGKIAEKV',\n","            'FLPLLAGLAASFLPTIFCKISRKC',\n","            'FFPIVAGVAAKVLKKIFCTISKKC',\n","    # AMP sequences, each of length 24\n","]\n","\n","# 2. Build a character-to-index mapping\n","#    In real data, you might have 20 canonical amino acids + special tokens if needed.\n","unique_amino_acids = sorted(list(set(\"\".join(amp_sequences))))\n","# e.g., unique_amino_acids might look like: [\"A\", \"C\", \"D\", \"E\", ..., \"Y\"]\n","\n","char_to_idx = {char: idx for idx, char in enumerate(unique_amino_acids)}\n","idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n","\n","vocab_size = len(unique_amino_acids)  # e.g., could be 20 if strictly canonical\n","\n","# 3. Convert sequences to integer arrays\n","encoded_sequences = []\n","for seq in amp_sequences:\n","    encoded_sequences.append([char_to_idx[c] for c in seq])\n","\n","encoded_sequences = np.array(encoded_sequences)  # shape: (num_sequences, seq_length)\n","\n","# 4. Prepare training data\n","#    We can train a \"next-character prediction\" model. Treat it like a tiime series.\n","#    For each position t in a sequence, predict the amino acid at position t+1.\n","#    We'll \"shift\" the sequence by 1 for targets.\n","#\n","#    Input: [X_0, X_1, ..., X_{22}],\n","#    Target: [X_1, X_2, ..., X_{23}].\n","#    We do this for all sequences.\n","\n","X = encoded_sequences[:, :-1]  # all but last character\n","y = encoded_sequences[:, 1:]   # all but first character\n","\n","# 5. Define LSTM model\n","'''\n","Sequential: This creates a linear stack of layers to build the LSTM model.\n","Embedding: This layer converts each amino acid index into a dense vector representation\n","  (embedding) of size embedding_dim. This allows the model to capture relationships between amino acids.\n","LSTM: This is the core layer, learning long-term dependencies in the sequence data. lstm_units sets the dimensionality of the LSTM's hidden state.\n","return_sequences=True makes the LSTM output a sequence for each input sequence,\n","  necessary for predicting the next amino acid at each position.\n","Dense: This is the output layer, with vocab_size neurons. It uses the 'softmax'\n","  activation to produce a probability distribution over all possible amino acids,\n","  representing the model's prediction for the next amino acid in the sequence.\n","Adam: An optimization algorithm that helps the model learn more effectively.\n","compile: Configures the model for training, specifying the loss function, optimizer, and evaluation metrics.\n","model.summary(): Prints a summary of the model's architecture.\n","'''\n","\n","model = Sequential()\n","# Embedding layer: (vocab_size) distinct amino acid characters -> embedding_dim vectors\n","embedding_dim = 8\n","model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=23))\n","\n","# LSTM layer\n","lstm_units = 64\n","model.add(LSTM(lstm_units, return_sequences=True))\n","\n","# Final Dense layer for classification over the vocabulary\n","model.add(Dense(vocab_size, activation='softmax'))\n","\n","optimizer = Adam(learning_rate=0.01)\n","model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n","\n","model.summary()\n","\n","# 6. Train the model\n","#    Note: Because the dataset is small, this is primarily an illustrative example.\n","'''\n","X and y: Represent the input and target data for training.\n","  X contains the encoded AMP sequences shifted by one position,\n","  and y contains the original encoded sequences shifted by one position to the right,\n","  so the model learns to predict the next amino acid in the sequence.\n","epochs: The number of times the model sees the entire training dataset.\n","batch_size: The number of samples processed before the model's internal parameters are updated.\n","model.fit: Starts the training process.\n","'''\n","\n","epochs = 50\n","batch_size = 16\n","model.fit(X, y, epochs=epochs, batch_size=batch_size)\n","\n","# 7. Generating new sequences\n","'''\n","generate_sequence: This function takes the trained model, a starting sequence (seed_seq),\n","  and a desired sequence length as input. It uses the model to predict the next amino acid step-by-step, generating a new sequence.\n","seed: The starting point for sequence generation, in this case, the amino acid 'F'.\n","The loop runs 20 times, generating and printing 20 new AMP sequences.\n","'''\n","def generate_sequence(model, seed_seq, length=24):\n","    \"\"\"\n","    Generate a new sequence of desired length using the trained model.\n","    :param model: trained LSTM model\n","    :param seed_seq: list of integer-encoded amino acids (starting sequence)\n","    :param length: desired total length of generated sequence\n","    :return: string of amino acids\n","    \"\"\"\n","    generated = seed_seq[:]  # copy\n","\n","    for _ in range(length - len(seed_seq)):\n","        # Predict next amino acid distribution\n","        input_seq = np.array(generated[-1:])  # last amino acid as input\n","        input_seq = input_seq.reshape(1, -1)  # shape: (1, 1)\n","\n","        # Model expects a fixed input length of 23 for each training example,\n","        # so for generation, we can adapt in different ways.\n","        # Simplest approach: pad/truncate to length=23 and only use last token for the next prediction\n","        # We'll do a simple approach:\n","        padded_seq = np.zeros((1, 23))\n","        padded_seq[0, 22] = input_seq[0, 0]\n","\n","        # Predict\n","        preds = model.predict(padded_seq, verbose=0)[0, 22, :]\n","\n","        next_idx = np.random.choice(range(vocab_size), p=preds)\n","        generated.append(next_idx)\n","\n","    # Convert generated integer tokens to string\n","    generated_str = \"\".join(idx_to_char[idx] for idx in generated)\n","    return generated_str\n","\n","# Example usage:\n","# Start generation from a single amino acid: 'F'\n","seed = [char_to_idx['F']]  # or choose any valid token from your vocab\n","for i in range(20):\n","    new_peptide = generate_sequence(model, seed, length=24)\n","    print(\"Generated Peptide:\", new_peptide)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"qcgiLLCZk4oh"},"execution_count":null,"outputs":[]}]}