{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPEMdGF4vGJkVYKqJVB360j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woldr001/AIChE_Workshop_MSU/blob/main/LSTM_AMPs_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generative ML Models in Protein Engineering"
      ],
      "metadata": {
        "id": "uOHGqsJLQF4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Generative Model for Antimicrobial Peptides\n",
        "================================================\n",
        "\n",
        "This script shows how to:\n",
        "\n",
        "1.   Preprocess AMP sequences (tokenize amino acids).\n",
        "2.   Train an LSTM-based model to predict the next amino acid.\n",
        "3.   Generate new sequences by sampling from the trained model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xzt4xP36qzmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "TNwqxr6Qr1nR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Antimicrobial Peptide (AMP) Sequence Dataset\n",
        "Note: With only ~150 AMP sequences (each length 24), overfitting is likely.\n",
        "      Consider data augmentation, dropout, or pretraining on larger protein sets."
      ],
      "metadata": {
        "id": "uJZUyR1WsBoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "amp_sequences = [\n",
        "            'FLPLLAGLAANFLPTIICKISYKC',\n",
        "            'FLPFIARLAAKVFPSIICSVTKKC',\n",
        "            'GVLSNVIGYLKKLGTGALNAVLKQ',\n",
        "            'GLFSVLGAVAKHVLPHVVPVIAEK',\n",
        "            'GLFKVLGSVAKHLLPHVAPVIAEK',\n",
        "            'GLFKVLGSVAKHLLPHVVPVIAEK',\n",
        "            'GLFGVLGSIAKHVLPHVVPVIAEK',\n",
        "            'MFFSSKKCKTVSKTFRGPCVRNAN',\n",
        "            'LLKELWTKMKGAGKAVLGKIKGLL',\n",
        "            'LLKELWTKIKGAGKAVLGKIKGLL',\n",
        "            'FWGALIKGAAKLIPSVVGLFKKKQ',\n",
        "            'FLPVVAGLAAKVLPSIICAVTKKC',\n",
        "            'FLPAIVGAAGQFLPKIFCAISKKC',\n",
        "            'FLPAIVGAAGKFLPKIFCAISKKC',\n",
        "            'FFPIVAGVAGQVLKKIYCTISKKC',\n",
        "            'FLPIIAGIAAKVFPKIFCAISKKC',\n",
        "            'FLPMLAGLAASMVPKLVCLITKKC',\n",
        "            'FLPMLAGLAASMVPKFVCLITKKC',\n",
        "            'FLPFIAGMAAKFLPKIFCAISKKC',\n",
        "            'FLPAIAGMAAKFLPKIFCAISKKC',\n",
        "            'FLPFIAGVAAKFLPKIFCAISKKC',\n",
        "            'FLPAIAGVAAKFLPKIFCAISKKC',\n",
        "            'FLPAIVGAAAKFLPKIFCVISKKC',\n",
        "            'FLPFIAGMAANFLPKIFCAISKKC',\n",
        "            'FLPIIAGVAAKVFPKIFCAISKKC',\n",
        "            'FLPIIASVAAKVFSKIFCAISKKC',\n",
        "            'FLPIIASVAANVFSKIFCAISKKC',\n",
        "            'GLNTLKKVFQGLHEAIKLINNHVQ',\n",
        "            'GLNALKKVFQGIHEAIKLINNHVQ',\n",
        "            'DSHAKRHHGYKRKFHEKHHSHRGY',\n",
        "            'FLPLLAGLAANFLPKIFCKITKKC',\n",
        "            'FLPILAGLAAKIVPKLFCLATKKC',\n",
        "            'FLPLIAGLAANFLPKIFCAITKKC',\n",
        "            'FLPVIAGVAAKFLPKIFCAITKKC',\n",
        "            'FWGALAKGALKLIPSLFSSFSKKD',\n",
        "            'ITSVSWCTPGCTSEGGGSGCSHCC',\n",
        "            'GLLNGLALRLGKRALKKIIKRLCR',\n",
        "            'ALWKDILKNAGKAALNEINQLVNQ',\n",
        "            'GLRSKIWLWVLLMIWQESNKFKKM',\n",
        "            'GKGRWLERIGKAGGIIIGGALDHL',\n",
        "            'FLGALIKGAIHGGRFIHGMIQNHH',\n",
        "            'FLGLLFHGVHHVGKWIHGLIHGHH',\n",
        "            'FLPMLAGLAANFLPKLFCKITKKC',\n",
        "            'FLPLAVSLAANFLPKLFCKITKKC',\n",
        "            'FLPLLAGLAANFFPKIFCKITRKC',\n",
        "            'FLPILASLAAKFGPKLFCLVTKKC',\n",
        "            'FLPILASLAAKLGPKLFCLVTKKC',\n",
        "            'FLPILASLAATLGPKLLCLITKKC',\n",
        "            'GIFSNMYARTPAGYFRGPAGYAAN',\n",
        "            'GLKDKFKSMGEKLKQYIQTWKAKF',\n",
        "            'SLKDKVKSMGEKLKQYIQTWKAKF',\n",
        "            'GFRDVLKGAAKAFVKTVAGHIANI',\n",
        "            'GIKDWIKGAAKKLIKTVASNIANQ',\n",
        "            'GFKDWIKGAAKKLIKTVASSIANQ',\n",
        "            'VIPFVASVAAEMMQHVYCAASKKC',\n",
        "            'FFGTALKIAANVLPTAICKILKKC',\n",
        "            'FFGTALKIAANILPTAICKILKKC',\n",
        "            'ILPFVAGVAAEMMQHVYCAASKKC',\n",
        "            'FLPAIVGAAAKFLPKIFCAISKKC',\n",
        "            'FLPIIAGVAAKVLPKIFCAISKKC',\n",
        "            'FLPIIAGIAAKFLPKIFCTISKKC',\n",
        "            'FLPVIAGVAANFLPKLFCAISKKC',\n",
        "            'FLPIIAGAAAKVVQKIFCAISKKC',\n",
        "            'FLPIIAGAAAKVVEKIFCAISKKC',\n",
        "            'FLPAVLRVAAKIVPTVFCAISKKC',\n",
        "            'FLPAVLRVAAQVVPTVFCAISKKC',\n",
        "            'FMGGLIKAATKIVPAAYCAITKKC',\n",
        "            'FLPILAGLAAKLVPKVFCSITKKC',\n",
        "            'FLPILAGLAANILPKVFCSITKKC',\n",
        "            'FFPIIAGMAAKLIPSLFCKITKKC',\n",
        "            'FMGSALRIAAKVLPAALCQIFKKC',\n",
        "            'DSHEKRHHEHRRKFHEKHHSHRGY',\n",
        "            'WRSLGRTLLRLSHALKPLARRSGW',\n",
        "            'VTSWSLCTPGCTSPGGGSNCSFCC',\n",
        "            'VIPFVASVAAEMMHHVYCAASKRC',\n",
        "            'SPAGCRFCCGCCPNMRGCGVCCRF',\n",
        "            'GRGREFMSNLKEKLSGVKEKMKNS',\n",
        "            'FLPVLTGLTPSIVPKLVCLLTKKC',\n",
        "            'FLPVLAGLTPSIVPKLVCLLTKKC',\n",
        "            'FFPMLAGVAARVVPKVICLITKKC',\n",
        "            'DSMGAVKLAKLLIDKMKCEVTKAC',\n",
        "            'FLPGVLRLVTKVGPAVVCAITRNC',\n",
        "            'VIVFVASVAAEMMQHVYCAASKKC',\n",
        "            'FLPAVIRVAANVLPTAFCAISKKC',\n",
        "            'IDPFVAGVAAEMMQHVYCAASKKC',\n",
        "            'INPFVAGVAAEMMQHVYCAASKKC',\n",
        "            'ILPFVAGVAAEMMKHVYCAASKKC',\n",
        "            'IIPFVAGVAAEMMEHVYCAASKKC',\n",
        "            'QLPFVAGVACEMCQCVYCAASKKC',\n",
        "            'ILPFVAGVAAEMMEHVYCAASKKC',\n",
        "            'ILPFVAGVAAMEMEHVYCAASKKC',\n",
        "            'FLPAVLLVATHVLPTVFCAITRKC',\n",
        "            'IPWKLPATFRPVERPFSKPFCRKD',\n",
        "            'FLPLLAGVVANFLPQIICKIARKC',\n",
        "            'FLGSLLGLVGKVVPTLFCKISKKC',\n",
        "            'FIGPVLKIAAGILPTAICKIFKKC',\n",
        "            'FVGPVLKIAAGILPTAICKIYKKC',\n",
        "            'FLGPIIKIATGILPTAICKFLKKC',\n",
        "            'FLPLIASLAANFVPKIFCKITKKC',\n",
        "            'FLPLIASVAANLVPKIFCKITKKC',\n",
        "            'FLSTLLKVAFKVVPTLFCPITKKC',\n",
        "            'KRKCPKTPFDNTPGAWFAHLILGC',\n",
        "            'FLGLIFHGLVHAGKLIHGLIHRNR',\n",
        "            'FLPAVIRVAANVLPTVFCAISKKC',\n",
        "            'FLPAVLRVAAKVVPTVFCLISKKC',\n",
        "            'FLSTALKVAANVVPTLFCKITKKC',\n",
        "            'FLPIVAGLAANFLPKIVCKITKKC',\n",
        "            'FLSTLLNVASNVVPTLICKITKKC',\n",
        "            'FLSTLLNVASKVVPTLFCKITKKC',\n",
        "            'FLPMLAGLAANFLPKIVCKITKKC',\n",
        "            'FIGPVLKMATSILPTAICKGFKKC',\n",
        "            'FLGPIIKMATGILPTAICKGLKKC',\n",
        "            'FLPIIAGVAAKVLPKLFCAITKKC',\n",
        "            'FLPVIAGLAAKVLPKLFCAITKKC',\n",
        "            'RKGWFKAMKSIAKFIAKEKLKEHL',\n",
        "            'FLPAVLKVAAHILPTAICAISRRC',\n",
        "            'FMGTALKIAANVLPAAFCKIFKKC',\n",
        "            'KLGFENFLVKALKTVMHVPTSPLL',\n",
        "            'GWLPTFGKILRKAMQLGPKLIQPI',\n",
        "            'GNGVVLTLTHECNLATWTKKLKCC',\n",
        "            'ITIPPIVKNTLKKFIKGAVSALMS',\n",
        "            'FLPGLIKAAVGVGSTILCKITKKC',\n",
        "            'FLPGLIKAAVGIGSTIFCKISKKC',\n",
        "            'FLPGLIKVAVGVGSTILCKITKKC',\n",
        "            'FLPGLIKAAVGIGSTIFCKISRKC',\n",
        "            'FLPMLAGLAANFLPKIICKITKKC',\n",
        "            'FLPIVASLAANFLPKIICKITKKC',\n",
        "            'FWGALAKGALKLIPSLVSSFTKKD',\n",
        "            'FFPLIAGLAARFLPKIFCSITKRC',\n",
        "            'VIPFVASVAAEMMQHVYCAASKRC',\n",
        "            'FFPSIAGLAAKFLPKIFCSITKRC',\n",
        "            'FLPAVLRVAAKVGPAVFCAITQKC',\n",
        "            'FLGMLLHGVGHAIHGLIHGKQNVE',\n",
        "            'NPAGCRFCCGCCPNMIGCGVCCRF',\n",
        "            'IWSFLIKAATKLLPSLFGGGKKDS',\n",
        "            'RNGCIVDPRCPYQQCRRPLYCRRR',\n",
        "            'ILELAGNAARDNKKTRIIPRHLQL',\n",
        "            'FLPLLAGLAANFLPTIICKIARKC',\n",
        "            'FLPAIIGMAAKVLPAFLCKITKKC',\n",
        "            'RRRRRFRRVIRRIRLPKYLTINTE',\n",
        "            'GNGVLKTISHECNMNTWQFLFTCC',\n",
        "            'FLPILAGLAANLVPKLICSITKKC',\n",
        "            'FLGAVLKVAGKLVPAAICKISKKC',\n",
        "            'FLGALFKVASKLVPAAICSISKKC',\n",
        "            'FLPVIAGIAANVLPKLFCKLTKRC',\n",
        "            'FFPIIARLAAKVIPSLVCAVTKKC',\n",
        "            'KRVNWRKVGRNTALGASYVLSFLG',\n",
        "            'GHSVDRIPEYFGPPGLPGPVLFYS',\n",
        "            'FLPLIAGVAAKVLPKIFCAISKKC',\n",
        "            'SDSVVSDIICTTFCSVTWCQSNCC',\n",
        "            'FLPLLAGLAANFLPQIICKIARKC',\n",
        "            'FLGTVLKVAAKVLPAALCQIFKKC',\n",
        "            'QSHLSMCRYCCCKGNKGCGFCCKF',\n",
        "            'VFDIIKDAGKQLVAHAMGKIAEKV',\n",
        "            'VFDIIKDAGRQLVAHAMGKIAEKV',\n",
        "            'FLPLLAGLAASFLPTIFCKISRKC',\n",
        "            'FFPIVAGVAAKVLKKIFCTISKKC',\n",
        "    # AMP sequences, each of length 24\n",
        "]"
      ],
      "metadata": {
        "id": "MYns1hrSsbD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Build a character-to-index mapping\n",
        "Depending on your data, you might have 20 canonical amino acids + special tokens if needed."
      ],
      "metadata": {
        "id": "K54IOXj0smTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_amino_acids = sorted(list(set(\"\".join(amp_sequences))))\n",
        "# e.g., unique_amino_acids might look like: [\"A\", \"C\", \"D\", \"E\", ..., \"Y\"]\n",
        "\n",
        "char_to_idx = {char: idx for idx, char in enumerate(unique_amino_acids)}\n",
        "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
        "\n",
        "vocab_size = len(unique_amino_acids)  # e.g., could be 20 if strictly canonical\n"
      ],
      "metadata": {
        "id": "8Gk_J2S-tD03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Convert sequences to integer arrays"
      ],
      "metadata": {
        "id": "pXRerRa-tb0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_sequences = []\n",
        "for seq in amp_sequences:\n",
        "    encoded_sequences.append([char_to_idx[c] for c in seq])\n",
        "\n",
        "encoded_sequences = np.array(encoded_sequences)  # shape: (num_sequences, seq_length)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1ZaHpW-PtcOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Prepare training data\n",
        "   We can train a \"next-character prediction\" model.   \n",
        "   We can treat the amino acid sequence as a tiime series.    \n",
        "   For each position t in a sequence, predict the amino acid at position t+1.       \n",
        "   We'll \"shift\" the sequence by 1 for targets.    \n",
        "   Input: [X_0, X_1, ..., X_{22}],   \n",
        "   Target: [X_1, X_2, ..., X_{23}].\n",
        "   \n",
        "   We do this for all sequences."
      ],
      "metadata": {
        "id": "vdOPgwV9tn1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = encoded_sequences[:, :-1]  # all but last character\n",
        "y = encoded_sequences[:, 1:]   # all but first character"
      ],
      "metadata": {
        "id": "iSVoze6-tsDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Define LSTM model\n",
        "\n",
        "**Sequential**: This creates a linear stack of layers to build the LSTM model.  \n",
        "\n",
        "**Embedding**: This layer converts each amino acid index into a dense vector representation (embedding) of size embedding_dim. This allows the model to capture relationships between amino acids.  \n",
        "\n",
        "**LSTM**: This is the core layer, learning long-term dependencies in the sequence data. lstm_units sets the dimensionality of the LSTM's hidden state.\n",
        "return_sequences=True makes the LSTM output a sequence for each input sequence,\n",
        "  necessary for predicting the next amino acid at each position.  \n",
        "\n",
        "**Dense**: This is the output layer, with vocab_size neurons. It uses the 'softmax'\n",
        "  activation to produce a probability distribution over all possible amino acids,\n",
        "  representing the model's prediction for the next amino acid in the sequence.  \n",
        "\n",
        "**Adam**: An optimization algorithm that helps the model learn more effectively.  \n",
        "\n",
        "**compile**: Configures the model for training, specifying the loss function, optimizer, and evaluation metrics.  \n",
        "\n",
        "**model.summary()**: Prints a summary of the model's architecture.\n"
      ],
      "metadata": {
        "id": "83hil2MtuhRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "# Embedding layer: (vocab_size) distinct amino acid characters -> embedding_dim vectors\n",
        "embedding_dim = 8\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=23))\n",
        "\n",
        "# LSTM layer\n",
        "lstm_units = 64\n",
        "model.add(LSTM(lstm_units, return_sequences=True))\n",
        "\n",
        "# Final Dense layer for classification over the vocabulary\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "FQZ0iwdPuhij",
        "outputId": "bbe65a6c-11a2-473e-de49-a5bbbce32e0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Train the model\n",
        "*Note: Because the dataset is small, this is primarily an illustrative example.*\n",
        "\n",
        "**X** and **y**: Represent the input and target data for training.\n",
        "  X contains the encoded AMP sequences shifted by one position,\n",
        "  and y contains the original encoded sequences shifted by one position to the right,\n",
        "  so the model learns to predict the next amino acid in the sequence.  \n",
        "**epochs**: The number of times the model sees the entire training dataset.  \n",
        "**batch_size**: The number of samples processed before the model's internal parameters are updated.  \n",
        "**model.fit**: Starts the training process.\n"
      ],
      "metadata": {
        "id": "9RDM9rczvWQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "batch_size = 16\n",
        "model.fit(X, y, epochs=epochs, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "11gg8CWnvWbs",
        "outputId": "b013d360-4248-4774-c866-9a52e1baca77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.1079 - loss: 2.8716\n",
            "Epoch 2/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.1861 - loss: 2.6210\n",
            "Epoch 3/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.2077 - loss: 2.5446\n",
            "Epoch 4/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.2239 - loss: 2.4219\n",
            "Epoch 5/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.2669 - loss: 2.3424\n",
            "Epoch 6/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.3628 - loss: 2.1469\n",
            "Epoch 7/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.4198 - loss: 1.9869\n",
            "Epoch 8/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.4685 - loss: 1.8082\n",
            "Epoch 9/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5069 - loss: 1.6442\n",
            "Epoch 10/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5227 - loss: 1.5588\n",
            "Epoch 11/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5269 - loss: 1.5127\n",
            "Epoch 12/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5555 - loss: 1.4529\n",
            "Epoch 13/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5459 - loss: 1.4696\n",
            "Epoch 14/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6004 - loss: 1.3135\n",
            "Epoch 15/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5997 - loss: 1.2828\n",
            "Epoch 16/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5787 - loss: 1.3528\n",
            "Epoch 17/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6170 - loss: 1.2312\n",
            "Epoch 18/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6482 - loss: 1.1235\n",
            "Epoch 19/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6602 - loss: 1.1149\n",
            "Epoch 20/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6559 - loss: 1.1241\n",
            "Epoch 21/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6703 - loss: 1.0575\n",
            "Epoch 22/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6832 - loss: 1.0316\n",
            "Epoch 23/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6944 - loss: 1.0098\n",
            "Epoch 24/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6975 - loss: 0.9841\n",
            "Epoch 25/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7271 - loss: 0.8907\n",
            "Epoch 26/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7493 - loss: 0.8147\n",
            "Epoch 27/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7782 - loss: 0.7528\n",
            "Epoch 28/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7642 - loss: 0.7878\n",
            "Epoch 29/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7794 - loss: 0.7816\n",
            "Epoch 30/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7960 - loss: 0.7183\n",
            "Epoch 31/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7793 - loss: 0.7402\n",
            "Epoch 32/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7873 - loss: 0.7057\n",
            "Epoch 33/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7959 - loss: 0.6750\n",
            "Epoch 34/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8166 - loss: 0.6256\n",
            "Epoch 35/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8138 - loss: 0.6243\n",
            "Epoch 36/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8375 - loss: 0.5491\n",
            "Epoch 37/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8480 - loss: 0.5426\n",
            "Epoch 38/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8555 - loss: 0.5205\n",
            "Epoch 39/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8562 - loss: 0.4996\n",
            "Epoch 40/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8570 - loss: 0.4895\n",
            "Epoch 41/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8671 - loss: 0.4763\n",
            "Epoch 42/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8573 - loss: 0.4845\n",
            "Epoch 43/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8607 - loss: 0.4728\n",
            "Epoch 44/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8679 - loss: 0.4677\n",
            "Epoch 45/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8635 - loss: 0.4497\n",
            "Epoch 46/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8851 - loss: 0.4077\n",
            "Epoch 47/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8839 - loss: 0.3983\n",
            "Epoch 48/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8967 - loss: 0.3853\n",
            "Epoch 49/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8934 - loss: 0.3697\n",
            "Epoch 50/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8986 - loss: 0.3589\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7eaa10b12dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Generating new sequences\n",
        "\n",
        "**generate_sequence**: This function takes the trained model, a starting sequence (seed_seq),\n",
        "  and a desired sequence length as input. It uses the model to predict the next amino acid step-by-step, generating a new sequence.  \n",
        "**seed**: The starting point for sequence generation, in this case, the amino acid 'F'.  \n",
        "The loop runs 20 times, generating and printing 20 new AMP sequences.\n"
      ],
      "metadata": {
        "id": "ogLMBfZOvsVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sequence(model, seed_seq, length=24):\n",
        "    \"\"\"\n",
        "    Generate a new sequence of desired length using the trained model.\n",
        "    :param model: trained LSTM model\n",
        "    :param seed_seq: list of integer-encoded amino acids (starting sequence)\n",
        "    :param length: desired total length of generated sequence\n",
        "    :return: string of amino acids\n",
        "    \"\"\"\n",
        "    generated = seed_seq[:]  # copy\n",
        "\n",
        "    for _ in range(length - len(seed_seq)):\n",
        "        # Predict next amino acid distribution\n",
        "        input_seq = np.array(generated[-1:])  # last amino acid as input\n",
        "        input_seq = input_seq.reshape(1, -1)  # shape: (1, 1)\n",
        "\n",
        "        # Model expects a fixed input length of 23 for each training example,\n",
        "        # so for generation, we can adapt in different ways.\n",
        "        # Simplest approach: pad/truncate to length=23 and only use last token for the next prediction\n",
        "        # We'll do a simple approach:\n",
        "        padded_seq = np.zeros((1, 23))\n",
        "        padded_seq[0, 22] = input_seq[0, 0]\n",
        "\n",
        "        # Predict\n",
        "        preds = model.predict(padded_seq, verbose=0)[0, 22, :]\n",
        "\n",
        "        next_idx = np.random.choice(range(vocab_size), p=preds)\n",
        "        generated.append(next_idx)\n",
        "\n",
        "    # Convert generated integer tokens to string\n",
        "    generated_str = \"\".join(idx_to_char[idx] for idx in generated)\n",
        "    return generated_str\n",
        "\n",
        "# Example usage:\n",
        "# Start generation from a single amino acid: 'F'\n",
        "seed = [char_to_idx['F']]  # or choose any valid token from your vocab\n",
        "for i in range(20):\n",
        "    new_peptide = generate_sequence(model, seed, length=24)\n",
        "    print(\"Generated Peptide:\", new_peptide)"
      ],
      "metadata": {
        "id": "NWkcJIZIvscB",
        "outputId": "77fb3fb2-e23e-46cc-f46f-4defe5f31ced",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Peptide: FTLDLIIIDIELIDDIIIDIIDID\n",
            "Generated Peptide: FDIIDIIIIIDIDIIDIIDIDDII\n",
            "Generated Peptide: FDIDIIDIIDDDVIIIDIDVDIDI\n",
            "Generated Peptide: FDIDIDIIDMIDIIDDIDIDIDLI\n",
            "Generated Peptide: FDIDLIIIDYDIIIDIDIIIDIDI\n",
            "Generated Peptide: FDWDLIIIIDIIDIIDIDIDIDID\n",
            "Generated Peptide: FDIDIDIDIDIIDDIDIIDIIDDI\n",
            "Generated Peptide: FDIIIDIDLDIRLDIIIIDIIFDI\n",
            "Generated Peptide: FNIIDIDDIIIIIDIDIDIDIIDI\n",
            "Generated Peptide: FDIDIIELIIDIIIIDIIIIDIDL\n",
            "Generated Peptide: FDDIDIIIDIDIIDIIDIDIDDID\n",
            "Generated Peptide: FDDIDIDIIDIIIDLNDILDIDLD\n",
            "Generated Peptide: FDIVDDIDIDDIIDIDDIIDIDID\n",
            "Generated Peptide: FDIDIWDIDIIELDDIDIIDIDLD\n",
            "Generated Peptide: FDIIDIDIDIIIIIDIDDIDDIID\n",
            "Generated Peptide: FDIDLIDIDIIDIDDIDIIDIDII\n",
            "Generated Peptide: FDIDIDIDIDIDIIDIDIIIIDII\n",
            "Generated Peptide: FDIIIDIIDMLDIIDIIIDIDIII\n",
            "Generated Peptide: FDLDIIIIDLIIRLFDIDIDFDSL\n",
            "Generated Peptide: FDIDIIDLDIVIDDIIIDIIDIYD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LSTM Generated Sequences:\n",
        "\n"
      ],
      "metadata": {
        "id": "_z7lj0AsrWqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "FLLRYYLRFRYLRFLRYLRYYYYL\n",
        "FLCRYYCRYYYLRRYCNYFLNLNL\n",
        "FLRFCRYYYLGYYLLRFRYYLRRY\n",
        "FLRYCRYYFGLLRYLRYLRYCRYL\n",
        "FLRYLRYYYYCRFLRFLRYCRYLR\n",
        "FYCRFRYLRKYLRYYYLRYYYRFR\n",
        "FCNRYRYRYYRYYYLLFRYYYYLR\n",
        "FLCRYCRYYYYYLRFSRFRRYYCR\n",
        "FCRYCRYRYLCLRCRRYYLRRYLR\n",
        "FLRLRLRRYLRLLRYCRFLRYYYL\n",
        "FRYRYLRRYYFYCRLCRYLRYCRY\n",
        "FCRYYYYRYCRYLRYYYLGYLRYL\n",
        "FRYYLRFLRFCRRLRYLCRCRYRY\n",
        "FLRYYYCRRYCRYCKYLGYCRRFR\n",
        "FCRYCRYMNYFLRFLRYRFRYYYF\n",
        "FRYYRYYYYYLLRYYYRRRCRYCR\n",
        "FCNLYCRFCRFLRCLRYYYCRYRY\n",
        "FCRRYLRYYYYYYYCRYLRYLRYY\n",
        "FCRYMNLRRRLRYYLCRYCRYRYR\n",
        "FLRRYYRCNRFLRYFYYLRYLRRY\n",
        "'''"
      ],
      "metadata": {
        "id": "qcgiLLCZk4oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformer Generative Model for Antimicrobial Peptides\n",
        "=======================================================\n",
        "\n",
        "This script demonstrates a simplified Transformer for generating short protein sequences.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TNx2SwNVBi3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n"
      ],
      "metadata": {
        "id": "ckbXoMRjanLS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Antimicrobial Peptide Sequence Database  \n",
        "**amp_sequences**: list of the AMP protein sequences the model will learn from."
      ],
      "metadata": {
        "id": "dMCcoLoianU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "amp_sequences = [\n",
        "            'FLPLLAGLAANFLPTIICKISYKC',\n",
        "            'FLPFIARLAAKVFPSIICSVTKKC',\n",
        "            'GVLSNVIGYLKKLGTGALNAVLKQ',\n",
        "            'GLFSVLGAVAKHVLPHVVPVIAEK',\n",
        "            'GLFKVLGSVAKHLLPHVAPVIAEK',\n",
        "            'GLFKVLGSVAKHLLPHVVPVIAEK',\n",
        "            'GLFGVLGSIAKHVLPHVVPVIAEK',\n",
        "            'MFFSSKKCKTVSKTFRGPCVRNAN',\n",
        "            'LLKELWTKMKGAGKAVLGKIKGLL',\n",
        "            'LLKELWTKIKGAGKAVLGKIKGLL',\n",
        "            'FWGALIKGAAKLIPSVVGLFKKKQ',\n",
        "            'FLPVVAGLAAKVLPSIICAVTKKC',\n",
        "            'FLPAIVGAAGQFLPKIFCAISKKC',\n",
        "            'FLPAIVGAAGKFLPKIFCAISKKC',\n",
        "            'FFPIVAGVAGQVLKKIYCTISKKC',\n",
        "            'FLPIIAGIAAKVFPKIFCAISKKC',\n",
        "            'FLPMLAGLAASMVPKLVCLITKKC',\n",
        "            'FLPMLAGLAASMVPKFVCLITKKC',\n",
        "            'FLPFIAGMAAKFLPKIFCAISKKC',\n",
        "            'FLPAIAGMAAKFLPKIFCAISKKC',\n",
        "            'FLPFIAGVAAKFLPKIFCAISKKC',\n",
        "            'FLPAIAGVAAKFLPKIFCAISKKC',\n",
        "            'FLPAIVGAAAKFLPKIFCVISKKC',\n",
        "            'FLPFIAGMAANFLPKIFCAISKKC',\n",
        "            'FLPIIAGVAAKVFPKIFCAISKKC',\n",
        "            'FLPIIASVAAKVFSKIFCAISKKC',\n",
        "            'FLPIIASVAANVFSKIFCAISKKC',\n",
        "            'GLNTLKKVFQGLHEAIKLINNHVQ',\n",
        "            'GLNALKKVFQGIHEAIKLINNHVQ',\n",
        "            'DSHAKRHHGYKRKFHEKHHSHRGY',\n",
        "            'FLPLLAGLAANFLPKIFCKITKKC',\n",
        "            'FLPILAGLAAKIVPKLFCLATKKC',\n",
        "            'FLPLIAGLAANFLPKIFCAITKKC',\n",
        "            'FLPVIAGVAAKFLPKIFCAITKKC',\n",
        "            'FWGALAKGALKLIPSLFSSFSKKD',\n",
        "            'ITSVSWCTPGCTSEGGGSGCSHCC',\n",
        "            'GLLNGLALRLGKRALKKIIKRLCR',\n",
        "            'ALWKDILKNAGKAALNEINQLVNQ',\n",
        "            'GLRSKIWLWVLLMIWQESNKFKKM',\n",
        "            'GKGRWLERIGKAGGIIIGGALDHL',\n",
        "            'FLGALIKGAIHGGRFIHGMIQNHH',\n",
        "            'FLGLLFHGVHHVGKWIHGLIHGHH',\n",
        "            'FLPMLAGLAANFLPKLFCKITKKC',\n",
        "            'FLPLAVSLAANFLPKLFCKITKKC',\n",
        "            'FLPLLAGLAANFFPKIFCKITRKC',\n",
        "            'FLPILASLAAKFGPKLFCLVTKKC',\n",
        "            'FLPILASLAAKLGPKLFCLVTKKC',\n",
        "            'FLPILASLAATLGPKLLCLITKKC',\n",
        "            'GIFSNMYARTPAGYFRGPAGYAAN',\n",
        "            'GLKDKFKSMGEKLKQYIQTWKAKF',\n",
        "            'SLKDKVKSMGEKLKQYIQTWKAKF',\n",
        "            'GFRDVLKGAAKAFVKTVAGHIANI',\n",
        "            'GIKDWIKGAAKKLIKTVASNIANQ',\n",
        "            'GFKDWIKGAAKKLIKTVASSIANQ',\n",
        "            'VIPFVASVAAEMMQHVYCAASKKC',\n",
        "            'FFGTALKIAANVLPTAICKILKKC',\n",
        "            'FFGTALKIAANILPTAICKILKKC',\n",
        "            'ILPFVAGVAAEMMQHVYCAASKKC',\n",
        "            'FLPAIVGAAAKFLPKIFCAISKKC',\n",
        "            'FLPIIAGVAAKVLPKIFCAISKKC',\n",
        "            'FLPIIAGIAAKFLPKIFCTISKKC',\n",
        "            'FLPVIAGVAANFLPKLFCAISKKC',\n",
        "            'FLPIIAGAAAKVVQKIFCAISKKC',\n",
        "            'FLPIIAGAAAKVVEKIFCAISKKC',\n",
        "            'FLPAVLRVAAKIVPTVFCAISKKC',\n",
        "            'FLPAVLRVAAQVVPTVFCAISKKC',\n",
        "            'FMGGLIKAATKIVPAAYCAITKKC',\n",
        "            'FLPILAGLAAKLVPKVFCSITKKC',\n",
        "            'FLPILAGLAANILPKVFCSITKKC',\n",
        "            'FFPIIAGMAAKLIPSLFCKITKKC',\n",
        "            'FMGSALRIAAKVLPAALCQIFKKC',\n",
        "            'DSHEKRHHEHRRKFHEKHHSHRGY',\n",
        "            'WRSLGRTLLRLSHALKPLARRSGW',\n",
        "            'VTSWSLCTPGCTSPGGGSNCSFCC',\n",
        "            'VIPFVASVAAEMMHHVYCAASKRC',\n",
        "            'SPAGCRFCCGCCPNMRGCGVCCRF',\n",
        "            'GRGREFMSNLKEKLSGVKEKMKNS',\n",
        "            'FLPVLTGLTPSIVPKLVCLLTKKC',\n",
        "            'FLPVLAGLTPSIVPKLVCLLTKKC',\n",
        "            'FFPMLAGVAARVVPKVICLITKKC',\n",
        "            'DSMGAVKLAKLLIDKMKCEVTKAC',\n",
        "            'FLPGVLRLVTKVGPAVVCAITRNC',\n",
        "            'VIVFVASVAAEMMQHVYCAASKKC',\n",
        "            'FLPAVIRVAANVLPTAFCAISKKC',\n",
        "            'IDPFVAGVAAEMMQHVYCAASKKC',\n",
        "            'INPFVAGVAAEMMQHVYCAASKKC',\n",
        "            'ILPFVAGVAAEMMKHVYCAASKKC',\n",
        "            'IIPFVAGVAAEMMEHVYCAASKKC',\n",
        "            'QLPFVAGVACEMCQCVYCAASKKC',\n",
        "            'ILPFVAGVAAEMMEHVYCAASKKC',\n",
        "            'ILPFVAGVAAMEMEHVYCAASKKC',\n",
        "            'FLPAVLLVATHVLPTVFCAITRKC',\n",
        "            'IPWKLPATFRPVERPFSKPFCRKD',\n",
        "            'FLPLLAGVVANFLPQIICKIARKC',\n",
        "            'FLGSLLGLVGKVVPTLFCKISKKC',\n",
        "            'FIGPVLKIAAGILPTAICKIFKKC',\n",
        "            'FVGPVLKIAAGILPTAICKIYKKC',\n",
        "            'FLGPIIKIATGILPTAICKFLKKC',\n",
        "            'FLPLIASLAANFVPKIFCKITKKC',\n",
        "            'FLPLIASVAANLVPKIFCKITKKC',\n",
        "            'FLSTLLKVAFKVVPTLFCPITKKC',\n",
        "            'KRKCPKTPFDNTPGAWFAHLILGC',\n",
        "            'FLGLIFHGLVHAGKLIHGLIHRNR',\n",
        "            'FLPAVIRVAANVLPTVFCAISKKC',\n",
        "            'FLPAVLRVAAKVVPTVFCLISKKC',\n",
        "            'FLSTALKVAANVVPTLFCKITKKC',\n",
        "            'FLPIVAGLAANFLPKIVCKITKKC',\n",
        "            'FLSTLLNVASNVVPTLICKITKKC',\n",
        "            'FLSTLLNVASKVVPTLFCKITKKC',\n",
        "            'FLPMLAGLAANFLPKIVCKITKKC',\n",
        "            'FIGPVLKMATSILPTAICKGFKKC',\n",
        "            'FLGPIIKMATGILPTAICKGLKKC',\n",
        "            'FLPIIAGVAAKVLPKLFCAITKKC',\n",
        "            'FLPVIAGLAAKVLPKLFCAITKKC',\n",
        "            'RKGWFKAMKSIAKFIAKEKLKEHL',\n",
        "            'FLPAVLKVAAHILPTAICAISRRC',\n",
        "            'FMGTALKIAANVLPAAFCKIFKKC',\n",
        "            'KLGFENFLVKALKTVMHVPTSPLL',\n",
        "            'GWLPTFGKILRKAMQLGPKLIQPI',\n",
        "            'GNGVVLTLTHECNLATWTKKLKCC',\n",
        "            'ITIPPIVKNTLKKFIKGAVSALMS',\n",
        "            'FLPGLIKAAVGVGSTILCKITKKC',\n",
        "            'FLPGLIKAAVGIGSTIFCKISKKC',\n",
        "            'FLPGLIKVAVGVGSTILCKITKKC',\n",
        "            'FLPGLIKAAVGIGSTIFCKISRKC',\n",
        "            'FLPMLAGLAANFLPKIICKITKKC',\n",
        "            'FLPIVASLAANFLPKIICKITKKC',\n",
        "            'FWGALAKGALKLIPSLVSSFTKKD',\n",
        "            'FFPLIAGLAARFLPKIFCSITKRC',\n",
        "            'VIPFVASVAAEMMQHVYCAASKRC',\n",
        "            'FFPSIAGLAAKFLPKIFCSITKRC',\n",
        "            'FLPAVLRVAAKVGPAVFCAITQKC',\n",
        "            'FLGMLLHGVGHAIHGLIHGKQNVE',\n",
        "            'NPAGCRFCCGCCPNMIGCGVCCRF',\n",
        "            'IWSFLIKAATKLLPSLFGGGKKDS',\n",
        "            'RNGCIVDPRCPYQQCRRPLYCRRR',\n",
        "            'ILELAGNAARDNKKTRIIPRHLQL',\n",
        "            'FLPLLAGLAANFLPTIICKIARKC',\n",
        "            'FLPAIIGMAAKVLPAFLCKITKKC',\n",
        "            'RRRRRFRRVIRRIRLPKYLTINTE',\n",
        "            'GNGVLKTISHECNMNTWQFLFTCC',\n",
        "            'FLPILAGLAANLVPKLICSITKKC',\n",
        "            'FLGAVLKVAGKLVPAAICKISKKC',\n",
        "            'FLGALFKVASKLVPAAICSISKKC',\n",
        "            'FLPVIAGIAANVLPKLFCKLTKRC',\n",
        "            'FFPIIARLAAKVIPSLVCAVTKKC',\n",
        "            'KRVNWRKVGRNTALGASYVLSFLG',\n",
        "            'GHSVDRIPEYFGPPGLPGPVLFYS',\n",
        "            'FLPLIAGVAAKVLPKIFCAISKKC',\n",
        "            'SDSVVSDIICTTFCSVTWCQSNCC',\n",
        "            'FLPLLAGLAANFLPQIICKIARKC',\n",
        "            'FLGTVLKVAAKVLPAALCQIFKKC',\n",
        "            'QSHLSMCRYCCCKGNKGCGFCCKF',\n",
        "            'VFDIIKDAGKQLVAHAMGKIAEKV',\n",
        "            'VFDIIKDAGRQLVAHAMGKIAEKV',\n",
        "            'FLPLLAGLAASFLPTIFCKISRKC',\n",
        "            'FFPIVAGVAAKVLKKIFCTISKKC',\n",
        "    # ...\n",
        "]\n"
      ],
      "metadata": {
        "id": "27zLJ-7SaneJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Build character-to-index mapping  \n",
        "**unique_amino_acids**: This extracts all the unique characters (amino acid letters) from the sequences.  \n",
        "**char_to_idx**: This dictionary maps each amino acid character to a unique numerical index (e.g., 'F' might be 0, 'L' might be 1, etc.).  \n",
        "**idx_to_char**: This dictionary does the reverse, mapping numerical indices back to amino acid characters.  \n",
        "**vocab_size**: This stores the total number of unique amino acids in the dataset."
      ],
      "metadata": {
        "id": "yDJ8ggtgannI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_amino_acids = sorted(list(set(\"\".join(amp_sequences))))\n",
        "char_to_idx = {char: idx for idx, char in enumerate(unique_amino_acids)}\n",
        "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
        "vocab_size = len(unique_amino_acids)"
      ],
      "metadata": {
        "id": "wemkDwzOanxB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Convert to integer arrays  \n",
        "**encoded_sequences**: This converts the original protein sequences (amp_sequences) into numerical representations using the **char_to_idx** mapping.\n",
        "  Each amino acid is replaced with its corresponding index.  \n",
        "**seq_length**: This sets the maximum length of the sequences the model will handle\n",
        "  (24 amino acids in this case).  \n",
        "**X** and **y**: These are created to train the model. **X** contains the input sequences\n",
        "  (all but the last amino acid), and y contains the target sequences (all but the first amino acid). This setup is for next-token prediction, where the model learns to predict the next amino acid in a sequence.  \n"
      ],
      "metadata": {
        "id": "fQgO5IL-an6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_sequences = []\n",
        "for seq in amp_sequences:\n",
        "    encoded_sequences.append([char_to_idx[c] for c in seq])\n",
        "encoded_sequences = np.array(encoded_sequences)  # shape: (num_sequences, seq_length)\n",
        "\n",
        "# Prepare training data for next-token prediction\n",
        "seq_length = 24\n",
        "X = encoded_sequences[:, :-1]  # shape: (num_sequences, seq_length-1)\n",
        "y = encoded_sequences[:, 1:]   # shape: (num_sequences, seq_length-1)\n",
        "\n"
      ],
      "metadata": {
        "id": "vFIhpctXbuDX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Build a small Transformer model  \n",
        "We'll define the input, embedding, transformer block, and final dense layer.  \n",
        "This section defines the architecture of the Transformer model using TensorFlow's Keras API.  \n",
        "**embedding_dim**, **num_heads**, **ff_dim**: These are hyperparameters that control the size and complexity of the model.\n",
        "The model consists of an input layer, an embedding layer (to represent amino acids as vectors),\n",
        "  a positional encoding layer (to provide information about the order of amino acids),\n",
        "  a transformer encoder block (the core of the model for learning relationships between amino acids),\n",
        "  and a final dense layer (to output predictions for the next amino acid).  \n",
        "**model.compile**: This configures the model for training, specifying the optimizer (adam),\n",
        "  loss function (sparse_categorical_crossentropy), and metrics to track (accuracy).  \n",
        "**model.summary()**: This displays a summary of the model's architecture.  "
      ],
      "metadata": {
        "id": "p6tkK6F0buU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 16\n",
        "num_heads = 2\n",
        "ff_dim = 32  # feed-forward layer size in transformer\n",
        "\n",
        "# Define Input\n",
        "inputs = layers.Input(shape=(seq_length-1,))  # each example is length-1 = 23\n",
        "\n",
        "# Token Embedding + Positional Embedding\n",
        "token_embedding = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)(inputs)\n",
        "\n",
        "# Basic positional encoding\n",
        "positions = tf.range(start=0, limit=seq_length-1, delta=1)\n",
        "positional_encoding = layers.Embedding(input_dim=seq_length, output_dim=embedding_dim)(positions)\n",
        "positional_encoding = positional_encoding[None, ...]  # shape: (1, seq_length-1, embedding_dim)\n",
        "\n",
        "# Add token embedding and positional encoding\n",
        "x = token_embedding + positional_encoding\n",
        "\n"
      ],
      "metadata": {
        "id": "TS5qXnEUcLgK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Transformer Encoder Block (simplified)  \n",
        "This is where self-attention is handled\n",
        "The model uses query, key, and attention weighting, although implicitly.  \n",
        "The **layers.MultiHeadAttention** layer handles these steps internally.  \n",
        "By passing x as both the query and the key/value (using (x, x)), the model is essentially performing self-attention, comparing different parts of the input sequence with itself.  \n",
        "The **key_dim** argument specifies the dimensionality of the keys and queries, influencing the complexity of the attention calculations.\n"
      ],
      "metadata": {
        "id": "Uvf4XNSfcLs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attention_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(x, x)\n",
        "attention_output = layers.Dropout(0.1)(attention_output)\n",
        "x = layers.LayerNormalization(epsilon=1e-6)(x + attention_output)\n",
        "\n",
        "ffn = layers.Dense(ff_dim, activation='relu')(x)\n",
        "ffn = layers.Dense(embedding_dim)(ffn)\n",
        "ffn = layers.Dropout(0.1)(ffn)\n",
        "x = layers.LayerNormalization(epsilon=1e-6)(x + ffn)\n",
        "\n",
        "# Final Dense Layer over vocab\n",
        "outputs = layers.Dense(vocab_size, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "qCmXh8GqcL0k",
        "outputId": "c1a64e26-7eec-4734-adba-f0a0816f8d24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │            \u001b[38;5;34m320\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add (\u001b[38;5;33mAdd\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │          \u001b[38;5;34m2,160\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],             │\n",
              "│                           │                        │                │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │             \u001b[38;5;34m32\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │            \u001b[38;5;34m544\u001b[0m │ layer_normalization[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │            \u001b[38;5;34m528\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_2 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ layer_normalization[\u001b[38;5;34m0\u001b[0m… │\n",
              "│                           │                        │                │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │             \u001b[38;5;34m32\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m20\u001b[0m)         │            \u001b[38;5;34m340\u001b[0m │ layer_normalization_1… │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,160</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],             │\n",
              "│                           │                        │                │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span> │ layer_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                           │                        │                │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">340</span> │ layer_normalization_1… │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,956\u001b[0m (15.45 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,956</span> (15.45 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,956\u001b[0m (15.45 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,956</span> (15.45 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Train the Transformer  \n",
        "\n",
        "**epochs**: The number of times the model will go through the entire training data.  \n",
        "**batch_size**: The number of training examples processed in each iteration.  \n",
        "**model.fit**: This starts the training process, using the prepared data (X, y) and the specified training parameters.  "
      ],
      "metadata": {
        "id": "g6QE6EKIcqJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "batch_size = 16\n",
        "model.fit(X, y, epochs=epochs, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "aZzZxy3gc138",
        "outputId": "21aa1fbc-f932-40eb-ec85-4c1daec1536a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.0679 - loss: 3.2779\n",
            "Epoch 2/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.1575 - loss: 2.8397\n",
            "Epoch 3/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2044 - loss: 2.6556\n",
            "Epoch 4/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2221 - loss: 2.6229\n",
            "Epoch 5/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2446 - loss: 2.5326\n",
            "Epoch 6/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2580 - loss: 2.4818\n",
            "Epoch 7/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2833 - loss: 2.4230\n",
            "Epoch 8/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3041 - loss: 2.3752\n",
            "Epoch 9/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3309 - loss: 2.3316\n",
            "Epoch 10/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3556 - loss: 2.3076\n",
            "Epoch 11/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3811 - loss: 2.2315\n",
            "Epoch 12/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3803 - loss: 2.2247\n",
            "Epoch 13/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3724 - loss: 2.2308\n",
            "Epoch 14/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4195 - loss: 2.0914\n",
            "Epoch 15/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4043 - loss: 2.1370\n",
            "Epoch 16/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4052 - loss: 2.1120\n",
            "Epoch 17/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4057 - loss: 2.0722\n",
            "Epoch 18/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4185 - loss: 2.0463\n",
            "Epoch 19/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4411 - loss: 2.0060\n",
            "Epoch 20/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4588 - loss: 1.9446\n",
            "Epoch 21/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4420 - loss: 1.9570\n",
            "Epoch 22/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4643 - loss: 1.8940\n",
            "Epoch 23/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4443 - loss: 1.9444\n",
            "Epoch 24/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4693 - loss: 1.9054\n",
            "Epoch 25/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4693 - loss: 1.8612\n",
            "Epoch 26/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4625 - loss: 1.8560\n",
            "Epoch 27/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4606 - loss: 1.8986\n",
            "Epoch 28/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4860 - loss: 1.7832\n",
            "Epoch 29/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4918 - loss: 1.7623\n",
            "Epoch 30/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4921 - loss: 1.7548\n",
            "Epoch 31/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4987 - loss: 1.7227\n",
            "Epoch 32/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5008 - loss: 1.7160\n",
            "Epoch 33/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5186 - loss: 1.6584\n",
            "Epoch 34/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5131 - loss: 1.6796\n",
            "Epoch 35/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4849 - loss: 1.7624\n",
            "Epoch 36/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4885 - loss: 1.7224\n",
            "Epoch 37/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.4890 - loss: 1.7318\n",
            "Epoch 38/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5164 - loss: 1.6354\n",
            "Epoch 39/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4836 - loss: 1.7334\n",
            "Epoch 40/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5228 - loss: 1.6083\n",
            "Epoch 41/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.4923 - loss: 1.7090\n",
            "Epoch 42/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5013 - loss: 1.6594\n",
            "Epoch 43/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5072 - loss: 1.6623\n",
            "Epoch 44/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.4886 - loss: 1.6911\n",
            "Epoch 45/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4939 - loss: 1.6612\n",
            "Epoch 46/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5253 - loss: 1.5671\n",
            "Epoch 47/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5335 - loss: 1.5892\n",
            "Epoch 48/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5464 - loss: 1.5281\n",
            "Epoch 49/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5041 - loss: 1.6067\n",
            "Epoch 50/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5117 - loss: 1.5894\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7eaa128a2a90>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. Generation function  \n",
        "**generate_transformer_sequence**: This function takes the trained model and a starting amino acid (**start_token**) and generates a new peptide sequence of the specified length.  \n",
        "It works by repeatedly predicting the next amino acid based on the previous ones, using the model's learned knowledge.  \n",
        "The example usage demonstrates how to generate 20 new sequences starting with 'F' and 20 starting with 'G'.  \n"
      ],
      "metadata": {
        "id": "1zPuqizYc4Fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_transformer_sequence(model, start_token, length=24):\n",
        "    \"\"\"\n",
        "    Generate a new peptide sequence from a transformer model.\n",
        "    :param model: trained Keras model\n",
        "    :param start_token: integer index of first amino acid\n",
        "    :param length: desired total length\n",
        "    :return: generated amino acid sequence (string)\n",
        "    \"\"\"\n",
        "    generated = [start_token]\n",
        "\n",
        "    for i in range(length-1):\n",
        "        # We feed the current sequence (minus 1 for next-token prediction)\n",
        "        input_seq = np.array(generated)[None, ...]  # shape: (1, current_length)\n",
        "\n",
        "        # Model expects length=23 for training; in generation we can adapt.\n",
        "        # We'll zero-pad to length=23 for simplicity (or you can dynamically mask).\n",
        "        pad_len = (seq_length - 1) - len(generated)\n",
        "        if pad_len < 0:\n",
        "            # If your sequence is already at length=23, we only use the last 23 tokens\n",
        "            input_seq = np.array(generated[-(seq_length-1):])[None, ...]\n",
        "            pad_len = 0\n",
        "\n",
        "        input_seq = np.pad(input_seq, ((0,0),(0,pad_len)), 'constant', constant_values=0)\n",
        "\n",
        "        preds = model.predict(input_seq, verbose=0)\n",
        "        # We want the last position's distribution\n",
        "        last_pos = len(generated)-1 if len(generated) < (seq_length-1) else (seq_length-2)\n",
        "        prob_dist = preds[0, last_pos]  # shape: (vocab_size,)\n",
        "\n",
        "        next_idx = np.random.choice(range(vocab_size), p=prob_dist)\n",
        "        generated.append(next_idx)\n",
        "\n",
        "    # Convert to string\n",
        "    generated_str = \"\".join(idx_to_char[idx] for idx in generated)\n",
        "    return generated_str"
      ],
      "metadata": {
        "id": "rtofii7oc4Og"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Transformer Generated AMP Sequences"
      ],
      "metadata": {
        "id": "0ldvnRdjdNhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(20):\n",
        "    start_token = char_to_idx['F']\n",
        "    new_peptide = generate_transformer_sequence(model, start_token, length=24)\n",
        "    print(\"Generated Peptide (Transformer):\", new_peptide)\n",
        "\n",
        "for i in range(20):\n",
        "    start_token = char_to_idx['G']\n",
        "    new_peptide = generate_transformer_sequence(model, start_token, length=24)\n",
        "    print(\"Generated Peptide (Transformer):\", new_peptide)"
      ],
      "metadata": {
        "id": "nmrCAu4sd8Hf",
        "outputId": "7e84c80b-edca-4566-fe34-af76850f2b06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Peptide (Transformer): FIPAIAAAAAAAAAAVFIPAACNF\n",
            "Generated Peptide (Transformer): FFFEIAAAAAPAAAAAACNESKAS\n",
            "Generated Peptide (Transformer): FYCAAADPAFNVAAYVPAPFPPCP\n",
            "Generated Peptide (Transformer): FKDSIAAAAPAFTAPIFCAIFKCC\n",
            "Generated Peptide (Transformer): FFFPIFGVAAAAAPAAFCAIAPFC\n",
            "Generated Peptide (Transformer): FFPAVAAAAAAVPAAAYCAATCFS\n",
            "Generated Peptide (Transformer): FADAVAAAAAAAAAAAYCAATAAP\n",
            "Generated Peptide (Transformer): FFPAAYCAAAAYCPTCCPCCPCCC\n",
            "Generated Peptide (Transformer): FVAAAAAAQAAGAAATAAAAAFAC\n",
            "Generated Peptide (Transformer): FFAYIPAAAAAFPAAAYCPYCCCC\n",
            "Generated Peptide (Transformer): FFDAFDAAAAAAPLAAFPAIPPVF\n",
            "Generated Peptide (Transformer): FCAAAAAAAAAAAAAIFFPFFPAA\n",
            "Generated Peptide (Transformer): FRVAAAGAAAEAAAAAIAIIAFFF\n",
            "Generated Peptide (Transformer): FFPAAAAAEFCAAAAYIPVIPPVC\n",
            "Generated Peptide (Transformer): FEEIFVAAAAAAAAAAIEAAAEAA\n",
            "Generated Peptide (Transformer): FFPANMVAAAEAAAAVFFPATAAY\n",
            "Generated Peptide (Transformer): FFFMIAAAAAAAACAPAAWHECQC\n",
            "Generated Peptide (Transformer): FIFAIAHAAAAAAAAFWCAAATAA\n",
            "Generated Peptide (Transformer): FVADFEIAAAFFFPAASFMITAAT\n",
            "Generated Peptide (Transformer): FYAFLAAAAAAYVAAMCAPFPPVF\n",
            "Generated Peptide (Transformer): GAAAFAAAAAAFAAAAYCAAAFFC\n",
            "Generated Peptide (Transformer): GETAAAAAAAAFVAAVYCAAFPAY\n",
            "Generated Peptide (Transformer): GVPAIFDTAAPAAPAAICAASWRC\n",
            "Generated Peptide (Transformer): GVLIAAAAAAAYLPAAAPAVIPMW\n",
            "Generated Peptide (Transformer): GVAARGFLAAACAAAAFCADAPSH\n",
            "Generated Peptide (Transformer): GAAGEAAAAAEHAAFVFAAAAAAA\n",
            "Generated Peptide (Transformer): GIDARAAAAAACAAAAICAFFLDC\n",
            "Generated Peptide (Transformer): GAAAVAAAAGAAAAAAYPAAAAEC\n",
            "Generated Peptide (Transformer): GFAAAAAAAAAFTVAAFCAEMTVA\n",
            "Generated Peptide (Transformer): GRPAAAAAAAAEECAVYCGVIPFC\n",
            "Generated Peptide (Transformer): GAAAIKAAAAAIVPHVFKAAEWND\n",
            "Generated Peptide (Transformer): GSAAAAAAAAFVAAAAVEAISKKC\n",
            "Generated Peptide (Transformer): GRVAFAAAAAHAAAAAVLPAAFYF\n",
            "Generated Peptide (Transformer): GVAAAAEIIALAAAAAAAAEEFFD\n",
            "Generated Peptide (Transformer): GAAAAAAVAAIAAPAAFCAAAVNC\n",
            "Generated Peptide (Transformer): GFPAIAAMAAFYAPAAPAMIPPAC\n",
            "Generated Peptide (Transformer): GAFPAPQIAAAYVPTARCAASKCC\n",
            "Generated Peptide (Transformer): GAAAIAPAAANDAPAAGPAASKKC\n",
            "Generated Peptide (Transformer): GAAAVAIVWVEMMAAMIAAAMMEG\n",
            "Generated Peptide (Transformer): GLFFPAAAAAAPAPAAGEFFYLFC\n"
          ]
        }
      ]
    }
  ]
}